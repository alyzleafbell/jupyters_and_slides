{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70453469, 0.6202538 , 0.42056889, 0.0337523 , 0.64946408,\n",
       "       0.25240364, 0.27005322, 0.78991391, 0.26520719, 0.30350205,\n",
       "       0.53537962, 0.89809374, 0.04971363, 0.98646059, 0.66913209,\n",
       "       0.07293736, 0.79674638, 0.3877931 , 0.77702124, 0.74592701])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear-Regression\n",
    "# 随机产生两系列数据\n",
    "import numpy as np\n",
    "import random\n",
    "random_data = np.random.random((20, 2))\n",
    "X = random_data[:,0]\n",
    "def assuming_function(x):\n",
    "    # 在我们的日常生活中是常见的\n",
    "    # 体重 -> 高血压的概率\n",
    "    # 收入 -> 买阿玛尼的概率\n",
    "    # 其实都是一种潜在的函数关系 + 一个随机变化\n",
    "    return 15.8 * x + 5+np.random.randint(-5,5)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [assuming_function(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD11JREFUeJzt3X+s3fVdx/HXi9LpVadl9kKgwu5mWLNmxHW5QQyJMpEV+8fKyGIgmcOEWDed0bg0gewPl/kHxLotMRK1ZgRmHG5qVxo3rQgs1WWgF4trGVYQmfa2oZdsRRKrlu7tH+fccnt3zznfnvP9cb7v7/OR3NxzvufbnveHc3ndbz+fz/fzcUQIANB+FzVdAACgHAQ6ACRBoANAEgQ6ACRBoANAEgQ6ACRBoANAEgQ6ACRBoANAEhfX+WYbN26Mubm5Ot8SAFrvqaeeejkiZkedV2ugz83NaWFhoc63BIDWs/3NIufR5QIASRDoAJAEgQ4ASRDoAJAEgQ4ASdQ6ywVAN+07tKjdB47q+KnTumLDjHZt26xbtm5quqx0CHQAldp3aFF37z2s02fOSpIWT53W3XsPSxKhXjK6XABUaveBo+fCfNnpM2e1+8DRhirKi0AHUKnjp05f0HGMj0AHUKkrNsxc0HGMj0AHUKld2zZrZv26847NrF+nXds2N1RRXgyKAqjU8sAns1yqR6ADqNwtWzcR4DWgywUAkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkhgZ6LavtP247WdtP2P71/rH32T7EdvP9b9fUn25AIBBilyhvybpoxHxdknXSfoV21sk3SXp0Yi4WtKj/ecAgIaMDPSIOBER/9R//KqkZyVtkrRD0oP90x6UdEtVRQIARrugPnTbc5K2SnpS0mURcULqhb6kSwf8mZ22F2wvLC0tTVYtAGCgwoFu+wck/YWkX4+I/yr65yJiT0TMR8T87OzsODUCAAootDiX7fXqhfmfRMTe/uGXbF8eESdsXy7pZFVFAmgn9hKtV5FZLpb0GUnPRsSnVry0X9Id/cd3SHq4/PIAtNXyXqKLp04r9PpeovsOLTZdWlpFulyul/Tzkn7a9tP9r+2S7pV0k+3nJN3Ufw4AkthLtAkju1wi4u8lecDLN5ZbDoAs2Eu0ftwpCqAS7CVaPwIdQCXYS7R+bEEHoBLsJVo/Ah1AZdhLtF50uQBAEgQ6ACRBoANAEgQ6ACRBoANAEgQ6ACRBoANAEgQ6ACRBoANAEgQ6ACTBrf9AAuwMBIlAB1pveWeg5c0klncGkkSodwxdLkDLsTMQlhHoQMuxMxCWEehAy7EzEJYR6EDLsTMQljEoCrQcOwNhGYEOJMDOQJDocgGANAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJLhTFJgSbFKBSRHomCpdDTU2qUAZCHRMjS6H2rBNKrK3PaOmLkzoQ8fU6PLOO2xSkcfyhcniqdMKvX5hsu/QYuXvTaBjanQ51NikIo8mL0wIdEyNLocam1Tk0eSFychAt32/7ZO2j6w49nHbi7af7n9tr7ZMdEGXQ+2WrZt0z63XaNOGGVnSpg0zuufWa+g/b6EmL0yKDIo+IOn3JH121fFPR8TvlF4ROqvrO++wSUUOu7ZtPm9wX6rvwmRkoEfEQdtzlVcCiFBD+zV5YTLJtMWP2P6gpAVJH42Ib5dUEwC0WlMXJuMOiv6+pB+V9E5JJyR9ctCJtnfaXrC9sLS0NObbAQBGGSvQI+KliDgbEd+R9EeSrh1y7p6ImI+I+dnZ2XHrBACMMFag2758xdP3SToy6FwAQD1G9qHbfkjSDZI22j4m6Tcl3WD7nZJC0ouSfqnCGgEABRSZ5XL7Goc/U0EtAFquq4urTQsW5wJQimleXK0rv2i49R9AKaZ1cbUmF8uqG4EOoBTTurjatP6iqQKBDqAU07q42rT+oqkCgQ6gFNO6uNq0/qKpAoEOoBTTumLktP6iqQKzXACUZhoXV+vSKp4EOoD0pvEXTRXocgGAJAh0AEiCLhfUoit36gFNItBRuWm+JRzIhC4XVK5Ld+oBTSLQUbku3akHNIlAR+W6dKce0CQCHZXr0p16QJMYFEXl2nynHrNz0CYEOmrRxjv1mJ2DtqHLBRiA2TloG67QgQGyzs6hGykvrtCBATLOzunSdmxdRKADA2ScnUM3Um50uQADtHl2ziDjdiPRTdMOBDowRBtn5wxzxYYZLa4R3sO6kZjt0x50uQAdMk43Et007cEVOtAh43QjZZ3tkxGBDnTMhXYjjdNNg2bQ5QK0yL5Di7r+3sf0lru+pOvvfayW6YYZZ/tkxRU60BJNDU5mnO2TFYEOtMSwwcmqwzXbbJ+s6HIBWoLBSYxCoAMtkXEpApSLQAdagsFJjEIfOtASDE5iFAIdaBEGJzEMXS4AkASBDgBJEOgAkMTIQLd9v+2Tto+sOPYm24/Yfq7//ZJqywQAjFLkCv0BSTevOnaXpEcj4mpJj/afAwAaNDLQI+KgpG+tOrxD0oP9xw9KuqXkugAAF2jcPvTLIuKEJPW/X1peSQCAcVQ+KGp7p+0F2wtLS0tVvx0AdNa4gf6S7cslqf/95KATI2JPRMxHxPzs7OyYbwcAGGXcQN8v6Y7+4zskPVxOOQCAcRWZtviQpK9J2mz7mO07Jd0r6Sbbz0m6qf8cANCgkWu5RMTtA166seRaAAAT4E5RAEiCQAeAJAh0AEii9euh7zu0yIL/AKCWB/q+Q4u6e+/hczuhL546rbv3HpYkQh1A57S6y2X3gaPnwnzZ6TNntfvA0YYqAoDmtDrQj586fUHHASCzVgf6FRtmLug4AGTW6kDftW2zZtavO+/YzPp12rVtc0MVAUBzWj0oujzwySwXAGh5oEu9UCfAAaDlXS4AgNcR6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEm0fi0XTIYt/IA8CPQOYws/IBe6XDqMLfyAXAj0DmMLPyAXAr3D2MIPyIVA7zC28ANyYVC0w9jCD8iFQO84tvAD8qDLBQCSINABIAkCHQCSINABIAkCHQCSINABIAkCHQCSYB56MiyHC3TXRIFu+0VJr0o6K+m1iJgvoyiMh+VwgW4r4wr93RHxcgl/D8aw8or8IltnI857fXk5XAIdyI8ulxZbfUW+OsyXsRwu0A2TDoqGpL+x/ZTtnWUUhOLW2qBiLSyHC3TDpFfo10fEcduXSnrE9r9ExMGVJ/SDfqckXXXVVRO+HVYqcuXNcrhAd0x0hR4Rx/vfT0r6oqRr1zhnT0TMR8T87OzsJG+HVQZdea+zZUmbNszonluvof8c6Iixr9Btf7+kiyLi1f7j90j6RGmVYaRd2zaf14cu9a7ICXGgmybpcrlM0hdtL/89n4uIvy6lKhTCBhUAVho70CPiBUk/VmItGMNaG1RwcxHQTUxbTIabi4DuYi2XZNaayrh8cxGA3Aj0ZAZNZeTmIiA/Aj2ZQVMZubkIyI9AT2bXts2aWb/uvGPcXAR0A4OiyTCVEeguAj2htaYyAsiPLhcASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkUi/Oxd6aALokbaCztyaArknb5cLemgC6Jm2gs7cmgK5JG+jsrQmga9IGOntrAuiatIOi7K0JoGvSBrrE3poAuiVtlwsAdA2BDgBJEOgAkASBDgBJEOgAkASBDgBJEOgAkESr5qGzHC4ADNaaQGc5XAAYrjVdLiyHCwDDtSbQWQ4XAIabKNBt32z7qO3nbd9VVlFrYTlcABhu7EC3vU7SfZJ+VtIWSbfb3lJWYauxHC4ADDfJoOi1kp6PiBckyfafStoh6RtlFLYay+ECwHCTBPomSf+54vkxST8+WTnDsRwuAAw2SR+61zgW33WSvdP2gu2FpaWlCd4OADDMJIF+TNKVK57/iKTjq0+KiD0RMR8R87OzsxO8HQBgmEkC/R8lXW37LbbfIOk2SfvLKQsAcKHG7kOPiNdsf0TSAUnrJN0fEc+UVhkA4IJMdOt/RHxZ0pdLqgUAMAFHfNc4ZnVvZi9J+uaAlzdKerm2YqYLbe+mrra9q+2Wxm/7myNi5CBkrYE+jO2FiJhvuo4m0Hba3iVdbbdUfdtbs5YLAGA4Ah0AkpimQN/TdAENou3d1NW2d7XdUsVtn5o+dADAZKbpCh0AMIHaA33UGuq2v8f25/uvP2l7ru4aq1Kg7b9h+xu2v277UdtvbqLOKhRdO9/2+22H7RSzIIq02/bP9T/3Z2x/ru4aq1Lg5/0q24/bPtT/md/eRJ1ls32/7ZO2jwx43bZ/t//f5eu231Xam0dEbV/q3VH6b5LeKukNkv5Z0pZV5/yypD/oP75N0ufrrLHhtr9b0vf1H3+4S23vn/dGSQclPSFpvum6a/rMr5Z0SNIl/eeXNl13jW3fI+nD/cdbJL3YdN0ltf0nJb1L0pEBr2+X9FfqLXB4naQny3rvuq/Qz62hHhH/J2l5DfWVdkh6sP/4zyXdaHutlR3bZmTbI+LxiPjv/tMn1FvwLIMin7sk/Zak35b0P3UWV6Ei7f5FSfdFxLclKSJO1lxjVYq0PST9YP/xD2mNxf3aKCIOSvrWkFN2SPps9DwhaYPty8t477oDfa011FcvcH7unIh4TdIrkn64luqqVaTtK92p3m/xDEa23fZWSVdGxF/WWVjFinzmb5P0Nttftf2E7Ztrq65aRdr+cUkfsH1MvSVEfrWe0hp3oVlQ2ERruYyhyBrqhdZZb6HC7bL9AUnzkn6q0orqM7Ttti+S9GlJv1BXQTUp8plfrF63yw3q/Yvs72y/IyJOVVxb1Yq0/XZJD0TEJ23/hKQ/7rf9O9WX16jKMq7uK/Qia6ifO8f2xer9U2zYP1/aotD68bZ/RtLHJL03Iv63ptqqNqrtb5T0Dklfsf2iev2K+xMMjBb9eX84Is5ExL9LOqpewLddkbbfKekLkhQRX5P0veqtdZJdoSwYR92BXmQN9f2S7ug/fr+kx6I/ktByI9ve73b4Q/XCPEtfqjSi7RHxSkRsjIi5iJhTb/zgvRGx0Ey5pSny875PvcFw2d6oXhfMC7VWWY0ibf8PSTdKku23qxfoXdjWbL+kD/Znu1wn6ZWIOFHK39zACPB2Sf+q3gj4x/rHPqHe/8BS70P9M0nPS/oHSW9tetS6xrb/raSXJD3d/9rfdM11tX3VuV9RglkuBT9zS/qUepurH5Z0W9M119j2LZK+qt4MmKclvafpmktq90OSTkg6o97V+J2SPiTpQys+8/v6/10Ol/mzzp2iAJAEd4oCQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAk8f/rfLRaBo0cSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6589227908087694"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X.reshape(-1, 1), y)\n",
    "reg.score(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a369905ac8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict model\n",
    "def fit(x): \n",
    "    return reg.coef_ * x + reg.intercept_\n",
    "# plot fit line\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X, fit(X), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.63725923])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reg.predict([[0.9]])==fit(0.9)\n",
    "reg.predict([[0.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from scipy.spatial.distance import cosine\n",
    "def model(X, y):\n",
    "    # 直接存储 X,y 即可\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]\n",
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)\n",
    "def predict(mymodel,x, k=3):\n",
    "    # 在predicate的时候，需要做大量的计算\n",
    "    most_similars = sorted(mymodel, key=lambda xi: distance(xi[0], x))[:k]\n",
    "    y_hats = [_y for x, _y in most_similars]\n",
    "    print(most_similars)\n",
    "    return np.mean(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.789913907257494, 19.480639734668408), (0.30350204596985153, 6.7953323263236545), (0.6691320945020902, 17.572287093133028)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.616086384708362"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn = model(X, y)\n",
    "predict(my_knn,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# entropy\n",
    "from collections import Counter \n",
    "from icecream import ic\n",
    "def entropy(elements):\n",
    "    '''群体的混乱程度'''\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal feature as the spilter of Decision Tree\n",
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_split_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            ic(sub_split_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_split_1)\n",
    "            ic(entropy_1)\n",
    "            \n",
    "            sub_split_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            ic(sub_split_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_split_2)\n",
    "            ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_split_1: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_split_2: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_split_1: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_split_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_split_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(training_data=dataset, target='bought')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_split_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_split_1: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_split_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_split_1: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_split_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '+10')\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '+10')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(dataset[dataset['family_number'] == 1], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_split_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| sub_split_1: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_split_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| sub_split_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_split_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.5623351446188083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 1)\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1 = dataset[dataset['family_number'] == 1]\n",
    "fm_n_1[fm_n_1['income'] == '+10']\n",
    "find_the_optimal_spilter(fm_n_1[fm_n_1['income'] == '+10'], 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a36a706470>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = [random.randint(0, 100) for _ in range(100)]\n",
    "X2 = [random.randint(0, 100) for _ in range(100)]\n",
    "plt.scatter(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81.25      , 68.8       ],\n",
       "       [13.72727273, 10.54545455],\n",
       "       [35.39130435, 84.30434783],\n",
       "       [88.72222222, 25.11111111],\n",
       "       [30.27777778, 47.61111111],\n",
       "       [46.4       , 15.1       ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranning_data = [[x1, x2] for x1, x2 in zip(X1, X2)]\n",
    "from sklearn.cluster import KMeans\n",
    "cluster = KMeans(n_clusters=6, max_iter=500)\n",
    "cluster.fit(tranning_data)\n",
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 2, 4, 4, 2, 2, 4, 2, 2, 4, 3, 2, 0, 2, 0, 1, 4, 0, 4, 4,\n",
       "       2, 2, 2, 2, 3, 4, 5, 5, 5, 0, 3, 4, 3, 3, 1, 0, 3, 0, 5, 3, 3, 5,\n",
       "       3, 5, 4, 5, 4, 0, 0, 3, 3, 1, 0, 2, 3, 0, 3, 2, 2, 0, 2, 3, 2, 0,\n",
       "       1, 1, 4, 0, 5, 3, 2, 2, 1, 0, 2, 4, 3, 1, 3, 0, 0, 1, 2, 5, 3, 4,\n",
       "       2, 0, 1, 0, 2, 1, 4, 0, 5, 1, 4, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, tranning_data):\n",
    "    centers[label].append(location)\n",
    "    \n",
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 模型是通过训练大量数据，从数据中获得“经验”，拟合实际的规律，用来预测新数据。没有模型是能完全描述实际的，但我们并不需要完美的模型，只要模型对实践有推动指导作用，就是有用的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Underfitting means bias is too much, while overfitting means variance is too much. \n",
    "Reasons for overfitting:Taking noises as features, memorize too much details of training data,while generalization is not enough.\n",
    "Reasons for underfitting: The ability of the learning model is weak, while the data is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- Precision：P=TP/(TP+FP) 精准率（查准率） 它表示的是预测为正的样本中有多少是真正的正样本;\n",
    "- Recall: R = R=TP/(TP+FN) 召回率（查全率 ） 它表示的是所有正样本中被预测准确的比率；\n",
    "- AUC: Area under curve(ROC).用TPR和FPR形成的曲线平滑度和该曲线下的面积大小，衡量模型的好坏\n",
    "- F1-score：2/(1/P+1/R)，其中的P指查准率，R指查全率，F1是两个率重要性相当的评估指标。 \n",
    "- F2-score: 来自Fbeta，beta=2, 此评估指标更适用于重视查全率的应用场景，比如癌症患者预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 机器学习就是通过学习历史数据，找出历史数据中的特征规律，获得“经验”，从而具备预测推断的智能，形成可预测新数据的模型。传统的分析式编程，所有的规律都需要提前内置在程序或算法中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans：这句话非常正确！评价模型好坏的指标众多，但不同的指标适用不同的场景，有的侧重查全率，有的适用精准度，如果能正确定义合适的评价标准，\n",
    "在模型测试过程中就能及时调整优化各类参数，否则，错误的评价标准可能会使得所有的调参南辕北辙。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perdicate based on Desicion Tree\n",
    "# loop findoptimalspliter(f,v)\n",
    "# filter dataset by (f,v),check labels left\n",
    "# if labels unique, that is the result\n",
    "# if loop end & labels not unique, choose the mode as the result\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "import pandas as pd\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "def predict(gender,income,family_number):\n",
    "    testdict={'gender':gender,'income':income,'family_number':family_number}\n",
    "    training_data=dataset\n",
    "    for i in range(len(testdict)):\n",
    "        (f,v) = find_the_optimal_spilter(training_data, target='bought')\n",
    "\n",
    "        if testdict[f]==v:\n",
    "            training_data = training_data[dataset[f]==v]\n",
    "        else:\n",
    "            training_data= training_data[dataset[f]!=v]\n",
    "        labels = set(training_data['bought'])\n",
    "        if len(labels)==1:\n",
    "            return list(labels)[0]\n",
    "    return Counter(training_data['bought']).most_common()[0][0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_split_1: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_split_2: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_split_1: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_split_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_split_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_split_1: [0, 0]\n",
      "ic| probs:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_split_1: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_split_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_split_1: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_split_2: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_split_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_split_2: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_split_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_split_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '+10')\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('F','10+',1)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([1,0,3,1,0,1]).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b\n",
    "# define loss function \n",
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat): \n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i in list(x):\n",
    "        gradient += abs(x_i)\n",
    "    return 1/n * gradient\n",
    "\n",
    "def partial_derivative_b():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 469.48941202046007, parameters k is 62.73548128484424 and b is 97.75265535477467\n",
      "Iteration 1, the loss is 469.44991539107735, parameters k is 62.72919665045689 and b is 97.75265535477467\n",
      "Iteration 2, the loss is 469.410418761695, parameters k is 62.72291201606954 and b is 97.75265535477467\n",
      "Iteration 3, the loss is 469.3709221323117, parameters k is 62.716627381682194 and b is 97.75265535477467\n",
      "Iteration 4, the loss is 469.3314255029297, parameters k is 62.710342747294845 and b is 97.75265535477467\n",
      "Iteration 5, the loss is 469.29192887354657, parameters k is 62.7040581129075 and b is 97.75265535477467\n",
      "Iteration 6, the loss is 469.25243224416414, parameters k is 62.69777347852015 and b is 97.75265535477467\n",
      "Iteration 7, the loss is 469.21293561478143, parameters k is 62.6914888441328 and b is 97.75265535477467\n",
      "Iteration 8, the loss is 469.17343898539866, parameters k is 62.68520420974545 and b is 97.75265535477467\n",
      "Iteration 9, the loss is 469.13394235601646, parameters k is 62.6789195753581 and b is 97.75265535477467\n",
      "Iteration 10, the loss is 469.09444572663335, parameters k is 62.67263494097075 and b is 97.75265535477467\n",
      "Iteration 11, the loss is 469.05494909724985, parameters k is 62.6663503065834 and b is 97.75265535477467\n",
      "Iteration 12, the loss is 469.0154524678682, parameters k is 62.660065672196055 and b is 97.75265535477467\n",
      "Iteration 13, the loss is 468.97595583848477, parameters k is 62.653781037808706 and b is 97.75265535477467\n",
      "Iteration 14, the loss is 468.9364592091028, parameters k is 62.64749640342136 and b is 97.75265535477467\n",
      "Iteration 15, the loss is 468.8969625797203, parameters k is 62.64121176903401 and b is 97.75265535477467\n",
      "Iteration 16, the loss is 468.8574659503371, parameters k is 62.63492713464666 and b is 97.75265535477467\n",
      "Iteration 17, the loss is 468.8179693209547, parameters k is 62.62864250025931 and b is 97.75265535477467\n",
      "Iteration 18, the loss is 468.7784726915723, parameters k is 62.62235786587196 and b is 97.75265535477467\n",
      "Iteration 19, the loss is 468.7389760621894, parameters k is 62.61607323148461 and b is 97.75265535477467\n",
      "Iteration 20, the loss is 468.6994794328061, parameters k is 62.609788597097264 and b is 97.75265535477467\n",
      "Iteration 21, the loss is 468.659982803424, parameters k is 62.603503962709915 and b is 97.75265535477467\n",
      "Iteration 22, the loss is 468.62048617404145, parameters k is 62.597219328322566 and b is 97.75265535477467\n",
      "Iteration 23, the loss is 468.58098954465845, parameters k is 62.59093469393522 and b is 97.75265535477467\n",
      "Iteration 24, the loss is 468.54149291527557, parameters k is 62.58465005954787 and b is 97.75265535477467\n",
      "Iteration 25, the loss is 468.5019962858929, parameters k is 62.57836542516052 and b is 97.75265535477467\n",
      "Iteration 26, the loss is 468.46249965651066, parameters k is 62.57208079077317 and b is 97.75265535477467\n",
      "Iteration 27, the loss is 468.42300302712806, parameters k is 62.56579615638582 and b is 97.75265535477467\n",
      "Iteration 28, the loss is 468.3835063977451, parameters k is 62.55951152199847 and b is 97.75265535477467\n",
      "Iteration 29, the loss is 468.3440097683629, parameters k is 62.553226887611125 and b is 97.75265535477467\n",
      "Iteration 30, the loss is 468.30451313897987, parameters k is 62.546942253223776 and b is 97.75265535477467\n",
      "Iteration 31, the loss is 468.2650165095974, parameters k is 62.54065761883643 and b is 97.75265535477467\n",
      "Iteration 32, the loss is 468.2255198802138, parameters k is 62.53437298444908 and b is 97.75265535477467\n",
      "Iteration 33, the loss is 468.186023250832, parameters k is 62.52808835006173 and b is 97.75265535477467\n",
      "Iteration 34, the loss is 468.14652662144937, parameters k is 62.52180371567438 and b is 97.75265535477467\n",
      "Iteration 35, the loss is 468.10702999206666, parameters k is 62.51551908128703 and b is 97.75265535477467\n",
      "Iteration 36, the loss is 468.0675333626841, parameters k is 62.50923444689968 and b is 97.75265535477467\n",
      "Iteration 37, the loss is 468.0280367333014, parameters k is 62.502949812512334 and b is 97.75265535477467\n",
      "Iteration 38, the loss is 467.98854010391824, parameters k is 62.496665178124985 and b is 97.75265535477467\n",
      "Iteration 39, the loss is 467.9490434745362, parameters k is 62.490380543737636 and b is 97.75265535477467\n",
      "Iteration 40, the loss is 467.909546845153, parameters k is 62.48409590935029 and b is 97.75265535477467\n",
      "Iteration 41, the loss is 467.8700502157709, parameters k is 62.47781127496294 and b is 97.75265535477467\n",
      "Iteration 42, the loss is 467.8305535863873, parameters k is 62.47152664057559 and b is 97.75265535477467\n",
      "Iteration 43, the loss is 467.7910569570056, parameters k is 62.46524200618824 and b is 97.75265535477467\n",
      "Iteration 44, the loss is 467.75156032762317, parameters k is 62.45895737180089 and b is 97.75265535477467\n",
      "Iteration 45, the loss is 467.71206369824074, parameters k is 62.45267273741354 and b is 97.75265535477467\n",
      "Iteration 46, the loss is 467.6725670688575, parameters k is 62.446388103026194 and b is 97.75265535477467\n",
      "Iteration 47, the loss is 467.63307043947486, parameters k is 62.440103468638846 and b is 97.75265535477467\n",
      "Iteration 48, the loss is 467.5935738100919, parameters k is 62.4338188342515 and b is 97.75265535477467\n",
      "Iteration 49, the loss is 467.55407718070904, parameters k is 62.42753419986415 and b is 97.75265535477467\n",
      "Iteration 50, the loss is 467.51458055132656, parameters k is 62.4212495654768 and b is 97.75265535477467\n",
      "Iteration 51, the loss is 467.4750839219444, parameters k is 62.41496493108945 and b is 97.75265535477467\n",
      "Iteration 52, the loss is 467.4355872925617, parameters k is 62.4086802967021 and b is 97.75265535477467\n",
      "Iteration 53, the loss is 467.3960906631784, parameters k is 62.40239566231475 and b is 97.75265535477467\n",
      "Iteration 54, the loss is 467.3565940337964, parameters k is 62.396111027927404 and b is 97.75265535477467\n",
      "Iteration 55, the loss is 467.31709740441335, parameters k is 62.389826393540055 and b is 97.75265535477467\n",
      "Iteration 56, the loss is 467.27760077503086, parameters k is 62.383541759152706 and b is 97.75265535477467\n",
      "Iteration 57, the loss is 467.2381041456483, parameters k is 62.37725712476536 and b is 97.75265535477467\n",
      "Iteration 58, the loss is 467.1986075162656, parameters k is 62.37097249037801 and b is 97.75265535477467\n",
      "Iteration 59, the loss is 467.1591108868825, parameters k is 62.36468785599066 and b is 97.75265535477467\n",
      "Iteration 60, the loss is 467.1196142574998, parameters k is 62.35840322160331 and b is 97.75265535477467\n",
      "Iteration 61, the loss is 467.08011762811765, parameters k is 62.35211858721596 and b is 97.75265535477467\n",
      "Iteration 62, the loss is 467.04062099873533, parameters k is 62.34583395282861 and b is 97.75265535477467\n",
      "Iteration 63, the loss is 467.0011243693521, parameters k is 62.339549318441264 and b is 97.75265535477467\n",
      "Iteration 64, the loss is 466.96162773996963, parameters k is 62.333264684053916 and b is 97.75265535477467\n",
      "Iteration 65, the loss is 466.922131110587, parameters k is 62.32698004966657 and b is 97.75265535477467\n",
      "Iteration 66, the loss is 466.88263448120455, parameters k is 62.32069541527922 and b is 97.75265535477467\n",
      "Iteration 67, the loss is 466.8431378518214, parameters k is 62.31441078089187 and b is 97.75265535477467\n",
      "Iteration 68, the loss is 466.8036412224389, parameters k is 62.30812614650452 and b is 97.75265535477467\n",
      "Iteration 69, the loss is 466.76414459305596, parameters k is 62.30184151211717 and b is 97.75265535477467\n",
      "Iteration 70, the loss is 466.7246479636732, parameters k is 62.29555687772982 and b is 97.75265535477467\n",
      "Iteration 71, the loss is 466.6851513342909, parameters k is 62.289272243342474 and b is 97.75265535477467\n",
      "Iteration 72, the loss is 466.64565470490817, parameters k is 62.282987608955125 and b is 97.75265535477467\n",
      "Iteration 73, the loss is 466.6061580755257, parameters k is 62.276702974567776 and b is 97.75265535477467\n",
      "Iteration 74, the loss is 466.5666614461428, parameters k is 62.27041834018043 and b is 97.75265535477467\n",
      "Iteration 75, the loss is 466.52716481676003, parameters k is 62.26413370579308 and b is 97.75265535477467\n",
      "Iteration 76, the loss is 466.4876681873774, parameters k is 62.25784907140573 and b is 97.75265535477467\n",
      "Iteration 77, the loss is 466.44817155799484, parameters k is 62.25156443701838 and b is 97.75265535477467\n",
      "Iteration 78, the loss is 466.40867492861184, parameters k is 62.24527980263103 and b is 97.75265535477467\n",
      "Iteration 79, the loss is 466.36917829922993, parameters k is 62.23899516824368 and b is 97.75265535477467\n",
      "Iteration 80, the loss is 466.3296816698468, parameters k is 62.232710533856334 and b is 97.75265535477467\n",
      "Iteration 81, the loss is 466.29018504046405, parameters k is 62.226425899468985 and b is 97.75265535477467\n",
      "Iteration 82, the loss is 466.2506884110816, parameters k is 62.22014126508164 and b is 97.75265535477467\n",
      "Iteration 83, the loss is 466.21119178169874, parameters k is 62.21385663069429 and b is 97.75265535477467\n",
      "Iteration 84, the loss is 466.17169515231603, parameters k is 62.20757199630694 and b is 97.75265535477467\n",
      "Iteration 85, the loss is 466.13219852293344, parameters k is 62.20128736191959 and b is 97.75265535477467\n",
      "Iteration 86, the loss is 466.09270189355055, parameters k is 62.19500272753224 and b is 97.75265535477467\n",
      "Iteration 87, the loss is 466.053205264168, parameters k is 62.18871809314489 and b is 97.75265535477467\n",
      "Iteration 88, the loss is 466.0137086347852, parameters k is 62.182433458757544 and b is 97.75265535477467\n",
      "Iteration 89, the loss is 465.97421200540253, parameters k is 62.176148824370195 and b is 97.75265535477467\n",
      "Iteration 90, the loss is 465.9347153760203, parameters k is 62.169864189982846 and b is 97.75265535477467\n",
      "Iteration 91, the loss is 465.8952187466374, parameters k is 62.1635795555955 and b is 97.75265535477467\n",
      "Iteration 92, the loss is 465.8557221172548, parameters k is 62.15729492120815 and b is 97.75265535477467\n",
      "Iteration 93, the loss is 465.8162254878723, parameters k is 62.1510102868208 and b is 97.75265535477467\n",
      "Iteration 94, the loss is 465.7767288584896, parameters k is 62.14472565243345 and b is 97.75265535477467\n",
      "Iteration 95, the loss is 465.7372322291067, parameters k is 62.1384410180461 and b is 97.75265535477467\n",
      "Iteration 96, the loss is 465.6977355997237, parameters k is 62.13215638365875 and b is 97.75265535477467\n",
      "Iteration 97, the loss is 465.65823897034176, parameters k is 62.125871749271404 and b is 97.75265535477467\n",
      "Iteration 98, the loss is 465.61874234095836, parameters k is 62.119587114884055 and b is 97.75265535477467\n",
      "Iteration 99, the loss is 465.5792457115761, parameters k is 62.113302480496706 and b is 97.75265535477467\n",
      "Iteration 100, the loss is 465.53974908219345, parameters k is 62.10701784610936 and b is 97.75265535477467\n",
      "Iteration 101, the loss is 465.5002524528112, parameters k is 62.10073321172201 and b is 97.75265535477467\n",
      "Iteration 102, the loss is 465.4607558234283, parameters k is 62.09444857733466 and b is 97.75265535477467\n",
      "Iteration 103, the loss is 465.4212591940454, parameters k is 62.08816394294731 and b is 97.75265535477467\n",
      "Iteration 104, the loss is 465.3817625646627, parameters k is 62.08187930855996 and b is 97.75265535477467\n",
      "Iteration 105, the loss is 465.3422659352802, parameters k is 62.07559467417261 and b is 97.75265535477467\n",
      "Iteration 106, the loss is 465.30276930589724, parameters k is 62.069310039785265 and b is 97.75265535477467\n",
      "Iteration 107, the loss is 465.2632726765148, parameters k is 62.063025405397916 and b is 97.75265535477467\n",
      "Iteration 108, the loss is 465.22377604713193, parameters k is 62.05674077101057 and b is 97.75265535477467\n",
      "Iteration 109, the loss is 465.1842794177497, parameters k is 62.05045613662322 and b is 97.75265535477467\n",
      "Iteration 110, the loss is 465.14478278836685, parameters k is 62.04417150223587 and b is 97.75265535477467\n",
      "Iteration 111, the loss is 465.1052861589843, parameters k is 62.03788686784852 and b is 97.75265535477467\n",
      "Iteration 112, the loss is 465.06578952960143, parameters k is 62.03160223346117 and b is 97.75265535477467\n",
      "Iteration 113, the loss is 465.02629290021895, parameters k is 62.02531759907382 and b is 97.75265535477467\n",
      "Iteration 114, the loss is 464.9867962708359, parameters k is 62.019032964686474 and b is 97.75265535477467\n",
      "Iteration 115, the loss is 464.94729964145324, parameters k is 62.012748330299125 and b is 97.75265535477467\n",
      "Iteration 116, the loss is 464.907803012071, parameters k is 62.006463695911776 and b is 97.75265535477467\n",
      "Iteration 117, the loss is 464.8683063826887, parameters k is 62.00017906152443 and b is 97.75265535477467\n",
      "Iteration 118, the loss is 464.8288097533055, parameters k is 61.99389442713708 and b is 97.75265535477467\n",
      "Iteration 119, the loss is 464.7893131239232, parameters k is 61.98760979274973 and b is 97.75265535477467\n",
      "Iteration 120, the loss is 464.7498164945405, parameters k is 61.98132515836238 and b is 97.75265535477467\n",
      "Iteration 121, the loss is 464.7103198651574, parameters k is 61.97504052397503 and b is 97.75265535477467\n",
      "Iteration 122, the loss is 464.67082323577506, parameters k is 61.96875588958768 and b is 97.75265535477467\n",
      "Iteration 123, the loss is 464.63132660639207, parameters k is 61.962471255200334 and b is 97.75265535477467\n",
      "Iteration 124, the loss is 464.5918299770099, parameters k is 61.956186620812986 and b is 97.75265535477467\n",
      "Iteration 125, the loss is 464.55233334762664, parameters k is 61.94990198642564 and b is 97.75265535477467\n",
      "Iteration 126, the loss is 464.51283671824393, parameters k is 61.94361735203829 and b is 97.75265535477467\n",
      "Iteration 127, the loss is 464.4733400888616, parameters k is 61.93733271765094 and b is 97.75265535477467\n",
      "Iteration 128, the loss is 464.43384345947885, parameters k is 61.93104808326359 and b is 97.75265535477467\n",
      "Iteration 129, the loss is 464.39434683009597, parameters k is 61.92476344887624 and b is 97.75265535477467\n",
      "Iteration 130, the loss is 464.35485020071326, parameters k is 61.91847881448889 and b is 97.75265535477467\n",
      "Iteration 131, the loss is 464.3153535713307, parameters k is 61.912194180101544 and b is 97.75265535477467\n",
      "Iteration 132, the loss is 464.27585694194823, parameters k is 61.905909545714195 and b is 97.75265535477467\n",
      "Iteration 133, the loss is 464.2363603125651, parameters k is 61.899624911326846 and b is 97.75265535477467\n",
      "Iteration 134, the loss is 464.1968636831829, parameters k is 61.8933402769395 and b is 97.75265535477467\n",
      "Iteration 135, the loss is 464.1573670538002, parameters k is 61.88705564255215 and b is 97.75265535477467\n",
      "Iteration 136, the loss is 464.11787042441733, parameters k is 61.8807710081648 and b is 97.75265535477467\n",
      "Iteration 137, the loss is 464.0783737950352, parameters k is 61.87448637377745 and b is 97.75265535477467\n",
      "Iteration 138, the loss is 464.03887716565237, parameters k is 61.8682017393901 and b is 97.75265535477467\n",
      "Iteration 139, the loss is 463.9993805362696, parameters k is 61.86191710500275 and b is 97.75265535477467\n",
      "Iteration 140, the loss is 463.9598839068867, parameters k is 61.855632470615404 and b is 97.75265535477467\n",
      "Iteration 141, the loss is 463.9203872775038, parameters k is 61.849347836228056 and b is 97.75265535477467\n",
      "Iteration 142, the loss is 463.88089064812164, parameters k is 61.84306320184071 and b is 97.75265535477467\n",
      "Iteration 143, the loss is 463.84139401873875, parameters k is 61.83677856745336 and b is 97.75265535477467\n",
      "Iteration 144, the loss is 463.80189738935576, parameters k is 61.83049393306601 and b is 97.75265535477467\n",
      "Iteration 145, the loss is 463.76240075997345, parameters k is 61.82420929867866 and b is 97.75265535477467\n",
      "Iteration 146, the loss is 463.722904130591, parameters k is 61.81792466429131 and b is 97.75265535477467\n",
      "Iteration 147, the loss is 463.683407501208, parameters k is 61.81164002990396 and b is 97.75265535477467\n",
      "Iteration 148, the loss is 463.6439108718254, parameters k is 61.805355395516614 and b is 97.75265535477467\n",
      "Iteration 149, the loss is 463.6044142424432, parameters k is 61.799070761129265 and b is 97.75265535477467\n",
      "Iteration 150, the loss is 463.5649176130601, parameters k is 61.792786126741916 and b is 97.75265535477467\n",
      "Iteration 151, the loss is 463.52542098367775, parameters k is 61.78650149235457 and b is 97.75265535477467\n",
      "Iteration 152, the loss is 463.485924354295, parameters k is 61.78021685796722 and b is 97.75265535477467\n",
      "Iteration 153, the loss is 463.44642772491227, parameters k is 61.77393222357987 and b is 97.75265535477467\n",
      "Iteration 154, the loss is 463.40693109553, parameters k is 61.76764758919252 and b is 97.75265535477467\n",
      "Iteration 155, the loss is 463.3674344661466, parameters k is 61.76136295480517 and b is 97.75265535477467\n",
      "Iteration 156, the loss is 463.32793783676397, parameters k is 61.75507832041782 and b is 97.75265535477467\n",
      "Iteration 157, the loss is 463.28844120738154, parameters k is 61.748793686030474 and b is 97.75265535477467\n",
      "Iteration 158, the loss is 463.2489445779986, parameters k is 61.742509051643125 and b is 97.75265535477467\n",
      "Iteration 159, the loss is 463.2094479486167, parameters k is 61.73622441725578 and b is 97.75265535477467\n",
      "Iteration 160, the loss is 463.16995131923335, parameters k is 61.72993978286843 and b is 97.75265535477467\n",
      "Iteration 161, the loss is 463.1304546898508, parameters k is 61.72365514848108 and b is 97.75265535477467\n",
      "Iteration 162, the loss is 463.0909580604681, parameters k is 61.71737051409373 and b is 97.75265535477467\n",
      "Iteration 163, the loss is 463.0514614310853, parameters k is 61.71108587970638 and b is 97.75265535477467\n",
      "Iteration 164, the loss is 463.01196480170296, parameters k is 61.70480124531903 and b is 97.75265535477467\n",
      "Iteration 165, the loss is 462.9724681723204, parameters k is 61.698516610931684 and b is 97.75265535477467\n",
      "Iteration 166, the loss is 462.9329715429378, parameters k is 61.692231976544335 and b is 97.75265535477467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 167, the loss is 462.8934749135549, parameters k is 61.685947342156986 and b is 97.75265535477467\n",
      "Iteration 168, the loss is 462.85397828417257, parameters k is 61.67966270776964 and b is 97.75265535477467\n",
      "Iteration 169, the loss is 462.8144816547891, parameters k is 61.67337807338229 and b is 97.75265535477467\n",
      "Iteration 170, the loss is 462.7749850254069, parameters k is 61.66709343899494 and b is 97.75265535477467\n",
      "Iteration 171, the loss is 462.73548839602427, parameters k is 61.66080880460759 and b is 97.75265535477467\n",
      "Iteration 172, the loss is 462.6959917666413, parameters k is 61.65452417022024 and b is 97.75265535477467\n",
      "Iteration 173, the loss is 462.6564951372585, parameters k is 61.64823953583289 and b is 97.75265535477467\n",
      "Iteration 174, the loss is 462.6169985078758, parameters k is 61.641954901445544 and b is 97.75265535477467\n",
      "Iteration 175, the loss is 462.57750187849314, parameters k is 61.635670267058195 and b is 97.75265535477467\n",
      "Iteration 176, the loss is 462.53800524911054, parameters k is 61.62938563267085 and b is 97.75265535477467\n",
      "Iteration 177, the loss is 462.49850861972817, parameters k is 61.6231009982835 and b is 97.75265535477467\n",
      "Iteration 178, the loss is 462.4590119903457, parameters k is 61.61681636389615 and b is 97.75265535477467\n",
      "Iteration 179, the loss is 462.41951536096286, parameters k is 61.6105317295088 and b is 97.75265535477467\n",
      "Iteration 180, the loss is 462.3800187315802, parameters k is 61.60424709512145 and b is 97.75265535477467\n",
      "Iteration 181, the loss is 462.3405221021973, parameters k is 61.5979624607341 and b is 97.75265535477467\n",
      "Iteration 182, the loss is 462.3010254728149, parameters k is 61.59167782634675 and b is 97.75265535477467\n",
      "Iteration 183, the loss is 462.26152884343173, parameters k is 61.585393191959405 and b is 97.75265535477467\n",
      "Iteration 184, the loss is 462.22203221404953, parameters k is 61.579108557572056 and b is 97.75265535477467\n",
      "Iteration 185, the loss is 462.18253558466694, parameters k is 61.57282392318471 and b is 97.75265535477467\n",
      "Iteration 186, the loss is 462.1430389552842, parameters k is 61.56653928879736 and b is 97.75265535477467\n",
      "Iteration 187, the loss is 462.1035423259016, parameters k is 61.56025465441001 and b is 97.75265535477467\n",
      "Iteration 188, the loss is 462.06404569651903, parameters k is 61.55397002002266 and b is 97.75265535477467\n",
      "Iteration 189, the loss is 462.02454906713587, parameters k is 61.54768538563531 and b is 97.75265535477467\n",
      "Iteration 190, the loss is 461.9850524377534, parameters k is 61.54140075124796 and b is 97.75265535477467\n",
      "Iteration 191, the loss is 461.9455558083713, parameters k is 61.535116116860614 and b is 97.75265535477467\n",
      "Iteration 192, the loss is 461.90605917898773, parameters k is 61.528831482473265 and b is 97.75265535477467\n",
      "Iteration 193, the loss is 461.86656254960525, parameters k is 61.522546848085916 and b is 97.75265535477467\n",
      "Iteration 194, the loss is 461.8270659202233, parameters k is 61.51626221369857 and b is 97.75265535477467\n",
      "Iteration 195, the loss is 461.78756929084034, parameters k is 61.50997757931122 and b is 97.75265535477467\n",
      "Iteration 196, the loss is 461.7480726614578, parameters k is 61.50369294492387 and b is 97.75265535477467\n",
      "Iteration 197, the loss is 461.7085760320754, parameters k is 61.49740831053652 and b is 97.75265535477467\n",
      "Iteration 198, the loss is 461.66907940269226, parameters k is 61.49112367614917 and b is 97.75265535477467\n",
      "Iteration 199, the loss is 461.6295827733096, parameters k is 61.48483904176182 and b is 97.75265535477467\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 200 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = 0#partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
