{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 课程代码复现\n",
    "- 前向后向神经网络深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "import numpy as np\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definition\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 167.316\n",
      "Epoch: 101, Loss: 7.856\n",
      "Epoch: 201, Loss: 4.705\n",
      "Epoch: 301, Loss: 4.288\n",
      "Epoch: 401, Loss: 4.622\n",
      "Epoch: 501, Loss: 3.862\n",
      "Epoch: 601, Loss: 4.191\n",
      "Epoch: 701, Loss: 3.823\n",
      "Epoch: 801, Loss: 3.480\n",
      "Epoch: 901, Loss: 4.028\n",
      "Epoch: 1001, Loss: 3.606\n",
      "Epoch: 1101, Loss: 3.477\n",
      "Epoch: 1201, Loss: 3.068\n",
      "Epoch: 1301, Loss: 3.157\n",
      "Epoch: 1401, Loss: 3.487\n",
      "Epoch: 1501, Loss: 3.653\n",
      "Epoch: 1601, Loss: 3.013\n",
      "Epoch: 1701, Loss: 3.256\n",
      "Epoch: 1801, Loss: 3.822\n",
      "Epoch: 1901, Loss: 2.743\n",
      "Epoch: 2001, Loss: 3.351\n",
      "Epoch: 2101, Loss: 3.125\n",
      "Epoch: 2201, Loss: 2.855\n",
      "Epoch: 2301, Loss: 3.103\n",
      "Epoch: 2401, Loss: 3.116\n",
      "Epoch: 2501, Loss: 3.195\n",
      "Epoch: 2601, Loss: 3.474\n",
      "Epoch: 2701, Loss: 2.904\n",
      "Epoch: 2801, Loss: 3.292\n",
      "Epoch: 2901, Loss: 2.748\n",
      "Epoch: 3001, Loss: 3.173\n",
      "Epoch: 3101, Loss: 3.568\n",
      "Epoch: 3201, Loss: 3.042\n",
      "Epoch: 3301, Loss: 2.740\n",
      "Epoch: 3401, Loss: 3.163\n",
      "Epoch: 3501, Loss: 2.735\n",
      "Epoch: 3601, Loss: 2.462\n",
      "Epoch: 3701, Loss: 2.810\n",
      "Epoch: 3801, Loss: 2.499\n",
      "Epoch: 3901, Loss: 3.287\n",
      "Epoch: 4001, Loss: 2.785\n",
      "Epoch: 4101, Loss: 2.701\n",
      "Epoch: 4201, Loss: 2.740\n",
      "Epoch: 4301, Loss: 2.447\n",
      "Epoch: 4401, Loss: 2.544\n",
      "Epoch: 4501, Loss: 2.826\n",
      "Epoch: 4601, Loss: 2.843\n",
      "Epoch: 4701, Loss: 2.911\n",
      "Epoch: 4801, Loss: 2.583\n",
      "Epoch: 4901, Loss: 2.677\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.1611133 ],\n",
       "       [16.03497528],\n",
       "       [18.90206633],\n",
       "       [21.69749677],\n",
       "       [19.00263486],\n",
       "       [14.59744693],\n",
       "       [24.26487229],\n",
       "       [24.26487229],\n",
       "       [19.94728885],\n",
       "       [16.27566602],\n",
       "       [16.82837549],\n",
       "       [10.98269915],\n",
       "       [46.5325388 ],\n",
       "       [47.17698836],\n",
       "       [11.26536297],\n",
       "       [18.27336168]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "506/506 [==============================] - 3s 6ms/step - loss: 159.8645 - mean_squared_error: 159.8645\n",
      "Epoch 2/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 67.6196 - mean_squared_error: 67.6196\n",
      "Epoch 3/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 51.3605 - mean_squared_error: 51.3605\n",
      "Epoch 4/5000\n",
      "506/506 [==============================] - 0s 129us/step - loss: 40.2001 - mean_squared_error: 40.2001\n",
      "Epoch 5/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 32.2447 - mean_squared_error: 32.2447\n",
      "Epoch 6/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 27.5698 - mean_squared_error: 27.5698\n",
      "Epoch 7/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 26.1618 - mean_squared_error: 26.1618\n",
      "Epoch 8/5000\n",
      "506/506 [==============================] - 0s 175us/step - loss: 23.8362 - mean_squared_error: 23.8362\n",
      "Epoch 9/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 21.9293 - mean_squared_error: 21.9293\n",
      "Epoch 10/5000\n",
      "506/506 [==============================] - 0s 129us/step - loss: 21.5011 - mean_squared_error: 21.5011\n",
      "Epoch 11/5000\n",
      "506/506 [==============================] - 0s 98us/step - loss: 20.1502 - mean_squared_error: 20.1502\n",
      "Epoch 12/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 20.2696 - mean_squared_error: 20.2696\n",
      "Epoch 13/5000\n",
      "506/506 [==============================] - 0s 185us/step - loss: 18.8054 - mean_squared_error: 18.8054\n",
      "Epoch 14/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 17.6324 - mean_squared_error: 17.6324\n",
      "Epoch 15/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 17.6500 - mean_squared_error: 17.6500\n",
      "Epoch 16/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 17.3355 - mean_squared_error: 17.3355\n",
      "Epoch 17/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 16.4009 - mean_squared_error: 16.4009\n",
      "Epoch 18/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 16.5616 - mean_squared_error: 16.5616\n",
      "Epoch 19/5000\n",
      "506/506 [==============================] - 0s 200us/step - loss: 15.6383 - mean_squared_error: 15.6383\n",
      "Epoch 20/5000\n",
      "506/506 [==============================] - 0s 199us/step - loss: 15.5222 - mean_squared_error: 15.5222\n",
      "Epoch 21/5000\n",
      "506/506 [==============================] - 0s 203us/step - loss: 15.4619 - mean_squared_error: 15.4619\n",
      "Epoch 22/5000\n",
      "506/506 [==============================] - 0s 213us/step - loss: 14.6792 - mean_squared_error: 14.6792\n",
      "Epoch 23/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 14.7744 - mean_squared_error: 14.7744\n",
      "Epoch 24/5000\n",
      "506/506 [==============================] - 0s 302us/step - loss: 14.5056 - mean_squared_error: 14.5056\n",
      "Epoch 25/5000\n",
      "506/506 [==============================] - 0s 396us/step - loss: 13.9896 - mean_squared_error: 13.9896\n",
      "Epoch 26/5000\n",
      "506/506 [==============================] - 0s 218us/step - loss: 14.2097 - mean_squared_error: 14.2097\n",
      "Epoch 27/5000\n",
      "506/506 [==============================] - 0s 217us/step - loss: 14.0094 - mean_squared_error: 14.0094\n",
      "Epoch 28/5000\n",
      "506/506 [==============================] - 0s 331us/step - loss: 13.8039 - mean_squared_error: 13.8039\n",
      "Epoch 29/5000\n",
      "506/506 [==============================] - 0s 199us/step - loss: 13.2943 - mean_squared_error: 13.2943\n",
      "Epoch 30/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 13.8751 - mean_squared_error: 13.8751\n",
      "Epoch 31/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 13.7776 - mean_squared_error: 13.7776\n",
      "Epoch 32/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 13.2305 - mean_squared_error: 13.2305\n",
      "Epoch 33/5000\n",
      "506/506 [==============================] - 0s 191us/step - loss: 12.6997 - mean_squared_error: 12.6997\n",
      "Epoch 34/5000\n",
      "506/506 [==============================] - 0s 250us/step - loss: 12.9530 - mean_squared_error: 12.9530\n",
      "Epoch 35/5000\n",
      "506/506 [==============================] - 0s 258us/step - loss: 13.0766 - mean_squared_error: 13.0766\n",
      "Epoch 36/5000\n",
      "506/506 [==============================] - 0s 237us/step - loss: 12.9664 - mean_squared_error: 12.9664\n",
      "Epoch 37/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 12.7434 - mean_squared_error: 12.7434\n",
      "Epoch 38/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 13.1249 - mean_squared_error: 13.1249\n",
      "Epoch 39/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 12.7779 - mean_squared_error: 12.7779\n",
      "Epoch 40/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 12.5588 - mean_squared_error: 12.5588\n",
      "Epoch 41/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 12.4381 - mean_squared_error: 12.4381\n",
      "Epoch 42/5000\n",
      "506/506 [==============================] - 0s 183us/step - loss: 12.4822 - mean_squared_error: 12.4822\n",
      "Epoch 43/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 12.2362 - mean_squared_error: 12.2362\n",
      "Epoch 44/5000\n",
      "506/506 [==============================] - 0s 181us/step - loss: 12.2441 - mean_squared_error: 12.2441\n",
      "Epoch 45/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 12.4250 - mean_squared_error: 12.4250\n",
      "Epoch 46/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 12.0991 - mean_squared_error: 12.0991\n",
      "Epoch 47/5000\n",
      "506/506 [==============================] - 0s 158us/step - loss: 11.7163 - mean_squared_error: 11.7163\n",
      "Epoch 48/5000\n",
      "506/506 [==============================] - 0s 177us/step - loss: 12.0034 - mean_squared_error: 12.0034\n",
      "Epoch 49/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 11.8285 - mean_squared_error: 11.8285\n",
      "Epoch 50/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 11.6621 - mean_squared_error: 11.6621\n",
      "Epoch 51/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 11.8573 - mean_squared_error: 11.8573\n",
      "Epoch 52/5000\n",
      "506/506 [==============================] - 0s 158us/step - loss: 11.5202 - mean_squared_error: 11.5202\n",
      "Epoch 53/5000\n",
      "506/506 [==============================] - 0s 153us/step - loss: 11.3331 - mean_squared_error: 11.3331\n",
      "Epoch 54/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 11.3154 - mean_squared_error: 11.3154\n",
      "Epoch 55/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 11.4546 - mean_squared_error: 11.4546\n",
      "Epoch 56/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 11.4016 - mean_squared_error: 11.4016\n",
      "Epoch 57/5000\n",
      "506/506 [==============================] - 0s 143us/step - loss: 11.2677 - mean_squared_error: 11.2677\n",
      "Epoch 58/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 10.8748 - mean_squared_error: 10.8748\n",
      "Epoch 59/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 11.1597 - mean_squared_error: 11.1597\n",
      "Epoch 60/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 11.2426 - mean_squared_error: 11.2426\n",
      "Epoch 61/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 10.3876 - mean_squared_error: 10.3876\n",
      "Epoch 62/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 10.6227 - mean_squared_error: 10.6227\n",
      "Epoch 63/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 10.8214 - mean_squared_error: 10.8214\n",
      "Epoch 64/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 10.4397 - mean_squared_error: 10.4397\n",
      "Epoch 65/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 11.0422 - mean_squared_error: 11.0422\n",
      "Epoch 66/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 10.6824 - mean_squared_error: 10.6824\n",
      "Epoch 67/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 10.0318 - mean_squared_error: 10.0318\n",
      "Epoch 68/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 10.5590 - mean_squared_error: 10.5590\n",
      "Epoch 69/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 10.1593 - mean_squared_error: 10.1593\n",
      "Epoch 70/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 10.0557 - mean_squared_error: 10.0557\n",
      "Epoch 71/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 99us/step - loss: 9.7945 - mean_squared_error: 9.7945\n",
      "Epoch 72/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 9.7737 - mean_squared_error: 9.7737\n",
      "Epoch 73/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 10.2635 - mean_squared_error: 10.2635\n",
      "Epoch 74/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 9.9962 - mean_squared_error: 9.9962\n",
      "Epoch 75/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 9.8292 - mean_squared_error: 9.8292\n",
      "Epoch 76/5000\n",
      "506/506 [==============================] - 0s 162us/step - loss: 9.6717 - mean_squared_error: 9.6717\n",
      "Epoch 77/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 9.9199 - mean_squared_error: 9.9199\n",
      "Epoch 78/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 9.5705 - mean_squared_error: 9.5705\n",
      "Epoch 79/5000\n",
      "506/506 [==============================] - 0s 198us/step - loss: 9.7992 - mean_squared_error: 9.7992\n",
      "Epoch 80/5000\n",
      "506/506 [==============================] - 0s 207us/step - loss: 9.6052 - mean_squared_error: 9.6052\n",
      "Epoch 81/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 9.4366 - mean_squared_error: 9.4366\n",
      "Epoch 82/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 9.0153 - mean_squared_error: 9.0153\n",
      "Epoch 83/5000\n",
      "506/506 [==============================] - 0s 169us/step - loss: 8.9686 - mean_squared_error: 8.9686\n",
      "Epoch 84/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 8.6234 - mean_squared_error: 8.6234\n",
      "Epoch 85/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 9.3774 - mean_squared_error: 9.3774\n",
      "Epoch 86/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 9.0097 - mean_squared_error: 9.0097\n",
      "Epoch 87/5000\n",
      "506/506 [==============================] - 0s 179us/step - loss: 8.9874 - mean_squared_error: 8.9874\n",
      "Epoch 88/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 8.8459 - mean_squared_error: 8.8459\n",
      "Epoch 89/5000\n",
      "506/506 [==============================] - 0s 197us/step - loss: 8.8247 - mean_squared_error: 8.8247\n",
      "Epoch 90/5000\n",
      "506/506 [==============================] - 0s 159us/step - loss: 8.9662 - mean_squared_error: 8.9662\n",
      "Epoch 91/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 8.6866 - mean_squared_error: 8.6866\n",
      "Epoch 92/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 8.7507 - mean_squared_error: 8.7507\n",
      "Epoch 93/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 9.1795 - mean_squared_error: 9.1795\n",
      "Epoch 94/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 8.9534 - mean_squared_error: 8.9534\n",
      "Epoch 95/5000\n",
      "506/506 [==============================] - 0s 226us/step - loss: 8.6524 - mean_squared_error: 8.6524\n",
      "Epoch 96/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 8.4343 - mean_squared_error: 8.4343\n",
      "Epoch 97/5000\n",
      "506/506 [==============================] - 0s 201us/step - loss: 8.3629 - mean_squared_error: 8.3629\n",
      "Epoch 98/5000\n",
      "506/506 [==============================] - 0s 174us/step - loss: 8.2228 - mean_squared_error: 8.2228\n",
      "Epoch 99/5000\n",
      "506/506 [==============================] - 0s 190us/step - loss: 8.3173 - mean_squared_error: 8.3173\n",
      "Epoch 100/5000\n",
      "506/506 [==============================] - 0s 170us/step - loss: 8.3192 - mean_squared_error: 8.3192\n",
      "Epoch 101/5000\n",
      "506/506 [==============================] - 0s 204us/step - loss: 8.3475 - mean_squared_error: 8.3475\n",
      "Epoch 102/5000\n",
      "506/506 [==============================] - 0s 297us/step - loss: 8.3762 - mean_squared_error: 8.3762\n",
      "Epoch 103/5000\n",
      "506/506 [==============================] - 0s 161us/step - loss: 8.1119 - mean_squared_error: 8.1119\n",
      "Epoch 104/5000\n",
      "506/506 [==============================] - 0s 169us/step - loss: 7.7901 - mean_squared_error: 7.7901\n",
      "Epoch 105/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 8.2972 - mean_squared_error: 8.2972\n",
      "Epoch 106/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 7.7942 - mean_squared_error: 7.7942\n",
      "Epoch 107/5000\n",
      "506/506 [==============================] - 0s 165us/step - loss: 8.0094 - mean_squared_error: 8.0094\n",
      "Epoch 108/5000\n",
      "506/506 [==============================] - 0s 167us/step - loss: 7.8720 - mean_squared_error: 7.8720\n",
      "Epoch 109/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 7.8377 - mean_squared_error: 7.8377\n",
      "Epoch 110/5000\n",
      "506/506 [==============================] - 0s 177us/step - loss: 7.8500 - mean_squared_error: 7.8500\n",
      "Epoch 111/5000\n",
      "506/506 [==============================] - 0s 155us/step - loss: 7.7997 - mean_squared_error: 7.7997\n",
      "Epoch 112/5000\n",
      "506/506 [==============================] - 0s 212us/step - loss: 7.6416 - mean_squared_error: 7.6416\n",
      "Epoch 113/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 7.6578 - mean_squared_error: 7.6578\n",
      "Epoch 114/5000\n",
      "506/506 [==============================] - 0s 188us/step - loss: 7.7457 - mean_squared_error: 7.7457\n",
      "Epoch 115/5000\n",
      "506/506 [==============================] - 0s 169us/step - loss: 7.7686 - mean_squared_error: 7.7686\n",
      "Epoch 116/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 7.7187 - mean_squared_error: 7.7187\n",
      "Epoch 117/5000\n",
      "506/506 [==============================] - 0s 195us/step - loss: 7.5403 - mean_squared_error: 7.5403\n",
      "Epoch 118/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 7.5135 - mean_squared_error: 7.5135\n",
      "Epoch 119/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 7.6027 - mean_squared_error: 7.6027\n",
      "Epoch 120/5000\n",
      "506/506 [==============================] - 0s 189us/step - loss: 7.6446 - mean_squared_error: 7.6446\n",
      "Epoch 121/5000\n",
      "506/506 [==============================] - 0s 182us/step - loss: 7.4563 - mean_squared_error: 7.4563\n",
      "Epoch 122/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 7.6297 - mean_squared_error: 7.62 - 0s 170us/step - loss: 7.1798 - mean_squared_error: 7.1798\n",
      "Epoch 123/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 7.5441 - mean_squared_error: 7.5441\n",
      "Epoch 124/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 7.3950 - mean_squared_error: 7.3950\n",
      "Epoch 125/5000\n",
      "506/506 [==============================] - 0s 199us/step - loss: 7.1683 - mean_squared_error: 7.1683\n",
      "Epoch 126/5000\n",
      "506/506 [==============================] - 0s 173us/step - loss: 7.1266 - mean_squared_error: 7.1266\n",
      "Epoch 127/5000\n",
      "506/506 [==============================] - 0s 171us/step - loss: 7.1324 - mean_squared_error: 7.1324\n",
      "Epoch 128/5000\n",
      "506/506 [==============================] - 0s 167us/step - loss: 6.9785 - mean_squared_error: 6.9785\n",
      "Epoch 129/5000\n",
      "506/506 [==============================] - 0s 98us/step - loss: 7.1559 - mean_squared_error: 7.1559\n",
      "Epoch 130/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 7.1615 - mean_squared_error: 7.1615\n",
      "Epoch 131/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 7.0053 - mean_squared_error: 7.0053\n",
      "Epoch 132/5000\n",
      "506/506 [==============================] - 0s 175us/step - loss: 6.9856 - mean_squared_error: 6.9856\n",
      "Epoch 133/5000\n",
      "506/506 [==============================] - 0s 181us/step - loss: 6.8607 - mean_squared_error: 6.8607\n",
      "Epoch 134/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 7.6523 - mean_squared_error: 7.6523\n",
      "Epoch 135/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 7.4417 - mean_squared_error: 7.4417\n",
      "Epoch 136/5000\n",
      "506/506 [==============================] - 0s 94us/step - loss: 6.8737 - mean_squared_error: 6.8737\n",
      "Epoch 137/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 6.8844 - mean_squared_error: 6.8844\n",
      "Epoch 138/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 6.7889 - mean_squared_error: 6.7889\n",
      "Epoch 139/5000\n",
      "506/506 [==============================] - 0s 200us/step - loss: 6.5773 - mean_squared_error: 6.5773\n",
      "Epoch 140/5000\n",
      "506/506 [==============================] - 0s 183us/step - loss: 6.5789 - mean_squared_error: 6.5789\n",
      "Epoch 141/5000\n",
      "506/506 [==============================] - 0s 203us/step - loss: 6.7213 - mean_squared_error: 6.7213\n",
      "Epoch 142/5000\n",
      "506/506 [==============================] - 0s 243us/step - loss: 6.6908 - mean_squared_error: 6.6908\n",
      "Epoch 143/5000\n",
      "506/506 [==============================] - 0s 275us/step - loss: 6.3949 - mean_squared_error: 6.3949\n",
      "Epoch 144/5000\n",
      "506/506 [==============================] - 0s 184us/step - loss: 6.4252 - mean_squared_error: 6.4252\n",
      "Epoch 145/5000\n",
      "506/506 [==============================] - 0s 162us/step - loss: 6.5510 - mean_squared_error: 6.5510\n",
      "Epoch 146/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 6.4553 - mean_squared_error: 6.4553\n",
      "Epoch 147/5000\n",
      "506/506 [==============================] - 0s 198us/step - loss: 6.3245 - mean_squared_error: 6.3245\n",
      "Epoch 148/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 6.3914 - mean_squared_error: 6.3914\n",
      "Epoch 149/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 6.6374 - mean_squared_error: 6.6374\n",
      "Epoch 150/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 6.2846 - mean_squared_error: 6.2846\n",
      "Epoch 151/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 6.3882 - mean_squared_error: 6.3882\n",
      "Epoch 152/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 6.3220 - mean_squared_error: 6.3220\n",
      "Epoch 153/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 6.2127 - mean_squared_error: 6.2127\n",
      "Epoch 154/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 6.3337 - mean_squared_error: 6.3337\n",
      "Epoch 155/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 6.1349 - mean_squared_error: 6.1349\n",
      "Epoch 156/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 6.1496 - mean_squared_error: 6.1496\n",
      "Epoch 157/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 6.2649 - mean_squared_error: 6.2649\n",
      "Epoch 158/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 6.2309 - mean_squared_error: 6.2309\n",
      "Epoch 159/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.9667 - mean_squared_error: 5.9667\n",
      "Epoch 160/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 6.4181 - mean_squared_error: 6.4181\n",
      "Epoch 161/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 6.1061 - mean_squared_error: 6.1061\n",
      "Epoch 162/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 6.0580 - mean_squared_error: 6.0580\n",
      "Epoch 163/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 6.2012 - mean_squared_error: 6.2012\n",
      "Epoch 164/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 6.3423 - mean_squared_error: 6.3423\n",
      "Epoch 165/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 6.1375 - mean_squared_error: 6.1375\n",
      "Epoch 166/5000\n",
      "506/506 [==============================] - 0s 156us/step - loss: 6.2061 - mean_squared_error: 6.2061\n",
      "Epoch 167/5000\n",
      "506/506 [==============================] - 0s 211us/step - loss: 5.8736 - mean_squared_error: 5.8736\n",
      "Epoch 168/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 5.9884 - mean_squared_error: 5.9884\n",
      "Epoch 169/5000\n",
      "506/506 [==============================] - 0s 189us/step - loss: 6.2325 - mean_squared_error: 6.2325\n",
      "Epoch 170/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 5.7784 - mean_squared_error: 5.7784\n",
      "Epoch 171/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 6.0525 - mean_squared_error: 6.0525\n",
      "Epoch 172/5000\n",
      "506/506 [==============================] - 0s 158us/step - loss: 5.8384 - mean_squared_error: 5.8384\n",
      "Epoch 173/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 5.7106 - mean_squared_error: 5.7106\n",
      "Epoch 174/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 6.0594 - mean_squared_error: 6.0594\n",
      "Epoch 175/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 6.0745 - mean_squared_error: 6.0745\n",
      "Epoch 176/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 5.6969 - mean_squared_error: 5.6969\n",
      "Epoch 177/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 5.7777 - mean_squared_error: 5.7777\n",
      "Epoch 178/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 5.8242 - mean_squared_error: 5.8242\n",
      "Epoch 179/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 5.7713 - mean_squared_error: 5.7713\n",
      "Epoch 180/5000\n",
      "506/506 [==============================] - 0s 173us/step - loss: 5.7056 - mean_squared_error: 5.7056\n",
      "Epoch 181/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 5.7001 - mean_squared_error: 5.7001\n",
      "Epoch 182/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 5.6780 - mean_squared_error: 5.6780\n",
      "Epoch 183/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 5.4728 - mean_squared_error: 5.4728\n",
      "Epoch 184/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.8917 - mean_squared_error: 5.8917\n",
      "Epoch 185/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 5.5088 - mean_squared_error: 5.5088\n",
      "Epoch 186/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 5.6817 - mean_squared_error: 5.6817\n",
      "Epoch 187/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 5.6930 - mean_squared_error: 5.6930\n",
      "Epoch 188/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 6.0252 - mean_squared_error: 6.0252\n",
      "Epoch 189/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 5.6000 - mean_squared_error: 5.6000\n",
      "Epoch 190/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 5.5041 - mean_squared_error: 5.5041\n",
      "Epoch 191/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 5.4171 - mean_squared_error: 5.4171\n",
      "Epoch 192/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.3532 - mean_squared_error: 5.3532\n",
      "Epoch 193/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 5.5839 - mean_squared_error: 5.5839\n",
      "Epoch 194/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 5.4973 - mean_squared_error: 5.4973\n",
      "Epoch 195/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 5.2276 - mean_squared_error: 5.2276\n",
      "Epoch 196/5000\n",
      "506/506 [==============================] - 0s 168us/step - loss: 5.7122 - mean_squared_error: 5.7122\n",
      "Epoch 197/5000\n",
      "506/506 [==============================] - 0s 191us/step - loss: 5.2101 - mean_squared_error: 5.2101\n",
      "Epoch 198/5000\n",
      "506/506 [==============================] - 0s 172us/step - loss: 5.3211 - mean_squared_error: 5.3211\n",
      "Epoch 199/5000\n",
      "506/506 [==============================] - 0s 174us/step - loss: 5.2564 - mean_squared_error: 5.2564\n",
      "Epoch 200/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.3793 - mean_squared_error: 5.3793\n",
      "Epoch 201/5000\n",
      "506/506 [==============================] - 0s 94us/step - loss: 5.4107 - mean_squared_error: 5.4107\n",
      "Epoch 202/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 5.3519 - mean_squared_error: 5.3519\n",
      "Epoch 203/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 5.2417 - mean_squared_error: 5.2417\n",
      "Epoch 204/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.2123 - mean_squared_error: 5.2123\n",
      "Epoch 205/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.1846 - mean_squared_error: 5.1846\n",
      "Epoch 206/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 5.3505 - mean_squared_error: 5.3505\n",
      "Epoch 207/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 5.3150 - mean_squared_error: 5.3150\n",
      "Epoch 208/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 5.1521 - mean_squared_error: 5.1521\n",
      "Epoch 209/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.1894 - mean_squared_error: 5.1894\n",
      "Epoch 210/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 5.1565 - mean_squared_error: 5.1565\n",
      "Epoch 211/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 158us/step - loss: 5.3433 - mean_squared_error: 5.3433\n",
      "Epoch 212/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 5.5846 - mean_squared_error: 5.58 - 0s 192us/step - loss: 5.3654 - mean_squared_error: 5.3654\n",
      "Epoch 213/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 5.1741 - mean_squared_error: 5.1741\n",
      "Epoch 214/5000\n",
      "506/506 [==============================] - 0s 115us/step - loss: 5.1091 - mean_squared_error: 5.1091\n",
      "Epoch 215/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 5.2326 - mean_squared_error: 5.2326\n",
      "Epoch 216/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 5.2085 - mean_squared_error: 5.2085\n",
      "Epoch 217/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 5.0256 - mean_squared_error: 5.0256\n",
      "Epoch 218/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.8789 - mean_squared_error: 4.8789\n",
      "Epoch 219/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.8679 - mean_squared_error: 4.8679\n",
      "Epoch 220/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.2074 - mean_squared_error: 5.2074\n",
      "Epoch 221/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.2304 - mean_squared_error: 5.2304\n",
      "Epoch 222/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.0814 - mean_squared_error: 5.0814\n",
      "Epoch 223/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 5.0942 - mean_squared_error: 5.0942\n",
      "Epoch 224/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 5.1107 - mean_squared_error: 5.1107\n",
      "Epoch 225/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.1889 - mean_squared_error: 5.1889\n",
      "Epoch 226/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 5.2216 - mean_squared_error: 5.2216\n",
      "Epoch 227/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.8908 - mean_squared_error: 4.8908\n",
      "Epoch 228/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.9403 - mean_squared_error: 4.9403\n",
      "Epoch 229/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.9603 - mean_squared_error: 4.9603\n",
      "Epoch 230/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.8108 - mean_squared_error: 4.8108\n",
      "Epoch 231/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.7919 - mean_squared_error: 4.7919\n",
      "Epoch 232/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.8389 - mean_squared_error: 4.8389\n",
      "Epoch 233/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.0410 - mean_squared_error: 5.0410\n",
      "Epoch 234/5000\n",
      "506/506 [==============================] - 0s 88us/step - loss: 4.8598 - mean_squared_error: 4.8598\n",
      "Epoch 235/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.8716 - mean_squared_error: 4.8716\n",
      "Epoch 236/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.7515 - mean_squared_error: 4.7515\n",
      "Epoch 237/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.8126 - mean_squared_error: 4.8126\n",
      "Epoch 238/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.6798 - mean_squared_error: 4.6798\n",
      "Epoch 239/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.7128 - mean_squared_error: 4.7128\n",
      "Epoch 240/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.0107 - mean_squared_error: 5.0107\n",
      "Epoch 241/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.8251 - mean_squared_error: 4.8251\n",
      "Epoch 242/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.7629 - mean_squared_error: 4.7629\n",
      "Epoch 243/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.7991 - mean_squared_error: 4.7991\n",
      "Epoch 244/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.6475 - mean_squared_error: 4.6475\n",
      "Epoch 245/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.5438 - mean_squared_error: 4.5438\n",
      "Epoch 246/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.6879 - mean_squared_error: 4.6879\n",
      "Epoch 247/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.6216 - mean_squared_error: 4.6216\n",
      "Epoch 248/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.7172 - mean_squared_error: 4.7172\n",
      "Epoch 249/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.7253 - mean_squared_error: 4.7253\n",
      "Epoch 250/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.7356 - mean_squared_error: 4.7356\n",
      "Epoch 251/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.4642 - mean_squared_error: 4.4642\n",
      "Epoch 252/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 4.5409 - mean_squared_error: 4.5409\n",
      "Epoch 253/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.5598 - mean_squared_error: 4.5598\n",
      "Epoch 254/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.5460 - mean_squared_error: 4.5460\n",
      "Epoch 255/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.5578 - mean_squared_error: 4.5578\n",
      "Epoch 256/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.5691 - mean_squared_error: 4.5691\n",
      "Epoch 257/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.4701 - mean_squared_error: 4.4701\n",
      "Epoch 258/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.6118 - mean_squared_error: 4.6118\n",
      "Epoch 259/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 4.3774 - mean_squared_error: 4.3774\n",
      "Epoch 260/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 4.4340 - mean_squared_error: 4.4340\n",
      "Epoch 261/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.4988 - mean_squared_error: 4.4988\n",
      "Epoch 262/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 4.7379 - mean_squared_error: 4.7379\n",
      "Epoch 263/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.3793 - mean_squared_error: 4.3793\n",
      "Epoch 264/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 4.5168 - mean_squared_error: 4.5168\n",
      "Epoch 265/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 4.3046 - mean_squared_error: 4.3046\n",
      "Epoch 266/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 4.3846 - mean_squared_error: 4.3846\n",
      "Epoch 267/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.3752 - mean_squared_error: 4.3752\n",
      "Epoch 268/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.2554 - mean_squared_error: 4.2554\n",
      "Epoch 269/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.6801 - mean_squared_error: 4.6801\n",
      "Epoch 270/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.4837 - mean_squared_error: 4.4837\n",
      "Epoch 271/5000\n",
      "506/506 [==============================] - 0s 98us/step - loss: 4.3428 - mean_squared_error: 4.3428\n",
      "Epoch 272/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.4975 - mean_squared_error: 4.4975\n",
      "Epoch 273/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.2886 - mean_squared_error: 4.2886\n",
      "Epoch 274/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 4.4331 - mean_squared_error: 4.4331\n",
      "Epoch 275/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.4034 - mean_squared_error: 4.4034\n",
      "Epoch 276/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.3845 - mean_squared_error: 4.3845\n",
      "Epoch 277/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.2434 - mean_squared_error: 4.2434\n",
      "Epoch 278/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 4.2241 - mean_squared_error: 4.2241\n",
      "Epoch 279/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.1633 - mean_squared_error: 4.1633\n",
      "Epoch 280/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.2747 - mean_squared_error: 4.2747\n",
      "Epoch 281/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.3614 - mean_squared_error: 4.3614\n",
      "Epoch 282/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.2188 - mean_squared_error: 4.2188\n",
      "Epoch 283/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.1989 - mean_squared_error: 4.1989\n",
      "Epoch 284/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 4.2201 - mean_squared_error: 4.2201\n",
      "Epoch 285/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.1909 - mean_squared_error: 4.1909\n",
      "Epoch 286/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.2769 - mean_squared_error: 4.2769\n",
      "Epoch 287/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.2396 - mean_squared_error: 4.2396\n",
      "Epoch 288/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.0828 - mean_squared_error: 4.0828\n",
      "Epoch 289/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.0933 - mean_squared_error: 4.0933\n",
      "Epoch 290/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.1371 - mean_squared_error: 4.1371\n",
      "Epoch 291/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.1325 - mean_squared_error: 4.1325\n",
      "Epoch 292/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 4.0718 - mean_squared_error: 4.0718\n",
      "Epoch 293/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.2737 - mean_squared_error: 4.2737\n",
      "Epoch 294/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.2005 - mean_squared_error: 4.2005\n",
      "Epoch 295/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.9910 - mean_squared_error: 3.9910\n",
      "Epoch 296/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 4.1208 - mean_squared_error: 4.1208\n",
      "Epoch 297/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.9953 - mean_squared_error: 3.9953\n",
      "Epoch 298/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.0454 - mean_squared_error: 4.0454\n",
      "Epoch 299/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.0901 - mean_squared_error: 4.0901\n",
      "Epoch 300/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 4.3308 - mean_squared_error: 4.3308\n",
      "Epoch 301/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.9773 - mean_squared_error: 3.9773\n",
      "Epoch 302/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.0577 - mean_squared_error: 4.0577\n",
      "Epoch 303/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 4.0209 - mean_squared_error: 4.0209\n",
      "Epoch 304/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.9297 - mean_squared_error: 3.9297\n",
      "Epoch 305/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 3.9895 - mean_squared_error: 3.9895\n",
      "Epoch 306/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.9909 - mean_squared_error: 3.9909\n",
      "Epoch 307/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 4.0892 - mean_squared_error: 4.0892\n",
      "Epoch 308/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 3.9814 - mean_squared_error: 3.9814\n",
      "Epoch 309/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 4.0397 - mean_squared_error: 4.0397\n",
      "Epoch 310/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.8441 - mean_squared_error: 3.8441\n",
      "Epoch 311/5000\n",
      "506/506 [==============================] - 0s 115us/step - loss: 3.9827 - mean_squared_error: 3.9827\n",
      "Epoch 312/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 3.9498 - mean_squared_error: 3.9498\n",
      "Epoch 313/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 3.9240 - mean_squared_error: 3.9240\n",
      "Epoch 314/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 3.8094 - mean_squared_error: 3.8094\n",
      "Epoch 315/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.9326 - mean_squared_error: 3.9326\n",
      "Epoch 316/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.6595 - mean_squared_error: 3.6595\n",
      "Epoch 317/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.9119 - mean_squared_error: 3.9119\n",
      "Epoch 318/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.8387 - mean_squared_error: 3.8387\n",
      "Epoch 319/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.9788 - mean_squared_error: 3.9788\n",
      "Epoch 320/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.8700 - mean_squared_error: 3.8700\n",
      "Epoch 321/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 3.7622 - mean_squared_error: 3.7622\n",
      "Epoch 322/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.7817 - mean_squared_error: 3.7817\n",
      "Epoch 323/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 3.7899 - mean_squared_error: 3.7899\n",
      "Epoch 324/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.8443 - mean_squared_error: 3.8443\n",
      "Epoch 325/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.8921 - mean_squared_error: 3.8921\n",
      "Epoch 326/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.8886 - mean_squared_error: 3.8886\n",
      "Epoch 327/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 3.8161 - mean_squared_error: 3.8161\n",
      "Epoch 328/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.7248 - mean_squared_error: 3.7248\n",
      "Epoch 329/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.8335 - mean_squared_error: 3.8335\n",
      "Epoch 330/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.7748 - mean_squared_error: 3.7748\n",
      "Epoch 331/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.7169 - mean_squared_error: 3.7169\n",
      "Epoch 332/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.9265 - mean_squared_error: 3.9265\n",
      "Epoch 333/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.7092 - mean_squared_error: 3.7092\n",
      "Epoch 334/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 3.7355 - mean_squared_error: 3.7355\n",
      "Epoch 335/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.6834 - mean_squared_error: 3.6834\n",
      "Epoch 336/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.6733 - mean_squared_error: 3.6733\n",
      "Epoch 337/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.6968 - mean_squared_error: 3.6968\n",
      "Epoch 338/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.8926 - mean_squared_error: 3.8926\n",
      "Epoch 339/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.6145 - mean_squared_error: 3.6145\n",
      "Epoch 340/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.7964 - mean_squared_error: 3.7964\n",
      "Epoch 341/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.6253 - mean_squared_error: 3.6253\n",
      "Epoch 342/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.6033 - mean_squared_error: 3.6033\n",
      "Epoch 343/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.5508 - mean_squared_error: 3.5508\n",
      "Epoch 344/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.7209 - mean_squared_error: 3.7209\n",
      "Epoch 345/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.6372 - mean_squared_error: 3.6372\n",
      "Epoch 346/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.5025 - mean_squared_error: 3.5025\n",
      "Epoch 347/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.6407 - mean_squared_error: 3.6407\n",
      "Epoch 348/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 3.6079 - mean_squared_error: 3.6079\n",
      "Epoch 349/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.5746 - mean_squared_error: 3.5746\n",
      "Epoch 350/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.5764 - mean_squared_error: 3.5764\n",
      "Epoch 351/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.5788 - mean_squared_error: 3.5788\n",
      "Epoch 352/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 97us/step - loss: 3.6656 - mean_squared_error: 3.6656\n",
      "Epoch 353/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.5847 - mean_squared_error: 3.5847\n",
      "Epoch 354/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.5873 - mean_squared_error: 3.5873\n",
      "Epoch 355/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.4939 - mean_squared_error: 3.4939\n",
      "Epoch 356/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.5489 - mean_squared_error: 3.5489\n",
      "Epoch 357/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.5398 - mean_squared_error: 3.5398\n",
      "Epoch 358/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.6437 - mean_squared_error: 3.6437\n",
      "Epoch 359/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.5445 - mean_squared_error: 3.5445\n",
      "Epoch 360/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.7718 - mean_squared_error: 3.7718\n",
      "Epoch 361/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.5328 - mean_squared_error: 3.5328\n",
      "Epoch 362/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.6019 - mean_squared_error: 3.6019\n",
      "Epoch 363/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.5053 - mean_squared_error: 3.5053\n",
      "Epoch 364/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 3.6366 - mean_squared_error: 3.6366\n",
      "Epoch 365/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.3884 - mean_squared_error: 3.3884\n",
      "Epoch 366/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.3275 - mean_squared_error: 3.3275\n",
      "Epoch 367/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.5537 - mean_squared_error: 3.5537\n",
      "Epoch 368/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 3.5049 - mean_squared_error: 3.5049\n",
      "Epoch 369/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.4289 - mean_squared_error: 3.4289\n",
      "Epoch 370/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 3.4183 - mean_squared_error: 3.4183\n",
      "Epoch 371/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.3815 - mean_squared_error: 3.3815\n",
      "Epoch 372/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 3.5261 - mean_squared_error: 3.5261\n",
      "Epoch 373/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 3.3791 - mean_squared_error: 3.3791\n",
      "Epoch 374/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.3924 - mean_squared_error: 3.3924\n",
      "Epoch 375/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 3.4220 - mean_squared_error: 3.4220\n",
      "Epoch 376/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 3.3454 - mean_squared_error: 3.3454\n",
      "Epoch 377/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.4364 - mean_squared_error: 3.4364\n",
      "Epoch 378/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.4495 - mean_squared_error: 3.4495\n",
      "Epoch 379/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.2869 - mean_squared_error: 3.2869\n",
      "Epoch 380/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.4210 - mean_squared_error: 3.4210\n",
      "Epoch 381/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.3778 - mean_squared_error: 3.3778\n",
      "Epoch 382/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 3.3128 - mean_squared_error: 3.3128\n",
      "Epoch 383/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.3617 - mean_squared_error: 3.3617\n",
      "Epoch 384/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 3.3330 - mean_squared_error: 3.3330\n",
      "Epoch 385/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 3.4230 - mean_squared_error: 3.4230\n",
      "Epoch 386/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 3.3398 - mean_squared_error: 3.3398\n",
      "Epoch 387/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 3.3656 - mean_squared_error: 3.3656\n",
      "Epoch 388/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 3.3216 - mean_squared_error: 3.3216\n",
      "Epoch 389/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 3.2348 - mean_squared_error: 3.2348\n",
      "Epoch 390/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.4011 - mean_squared_error: 3.4011\n",
      "Epoch 391/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.2769 - mean_squared_error: 3.2769\n",
      "Epoch 392/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.3427 - mean_squared_error: 3.3427\n",
      "Epoch 393/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.1887 - mean_squared_error: 3.1887\n",
      "Epoch 394/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.2912 - mean_squared_error: 3.2912\n",
      "Epoch 395/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.2824 - mean_squared_error: 3.2824\n",
      "Epoch 396/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.3255 - mean_squared_error: 3.3255\n",
      "Epoch 397/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 3.1692 - mean_squared_error: 3.1692\n",
      "Epoch 398/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.3611 - mean_squared_error: 3.3611\n",
      "Epoch 399/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.2829 - mean_squared_error: 3.2829\n",
      "Epoch 400/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 3.2977 - mean_squared_error: 3.2977\n",
      "Epoch 401/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.2534 - mean_squared_error: 3.2534\n",
      "Epoch 402/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 3.1730 - mean_squared_error: 3.1730\n",
      "Epoch 403/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.1889 - mean_squared_error: 3.1889\n",
      "Epoch 404/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.1461 - mean_squared_error: 3.1461\n",
      "Epoch 405/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.1712 - mean_squared_error: 3.1712\n",
      "Epoch 406/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.0925 - mean_squared_error: 3.0925\n",
      "Epoch 407/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.1273 - mean_squared_error: 3.1273\n",
      "Epoch 408/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.1576 - mean_squared_error: 3.1576\n",
      "Epoch 409/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.1536 - mean_squared_error: 3.1536\n",
      "Epoch 410/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 3.0887 - mean_squared_error: 3.0887\n",
      "Epoch 411/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.1583 - mean_squared_error: 3.1583\n",
      "Epoch 412/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 3.2042 - mean_squared_error: 3.2042\n",
      "Epoch 413/5000\n",
      "506/506 [==============================] - 0s 129us/step - loss: 3.1194 - mean_squared_error: 3.1194\n",
      "Epoch 414/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.1062 - mean_squared_error: 3.1062\n",
      "Epoch 415/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.0512 - mean_squared_error: 3.0512\n",
      "Epoch 416/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 3.1647 - mean_squared_error: 3.1647\n",
      "Epoch 417/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 3.1601 - mean_squared_error: 3.1601\n",
      "Epoch 418/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.1142 - mean_squared_error: 3.1142\n",
      "Epoch 419/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.0845 - mean_squared_error: 3.0845\n",
      "Epoch 420/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.1161 - mean_squared_error: 3.1161\n",
      "Epoch 421/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.0264 - mean_squared_error: 3.0264\n",
      "Epoch 422/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.1476 - mean_squared_error: 3.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.1413 - mean_squared_error: 3.1413\n",
      "Epoch 424/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.9519 - mean_squared_error: 2.9519\n",
      "Epoch 425/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.1217 - mean_squared_error: 3.1217\n",
      "Epoch 426/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.0707 - mean_squared_error: 3.0707\n",
      "Epoch 427/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.9731 - mean_squared_error: 2.9731\n",
      "Epoch 428/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 3.2507 - mean_squared_error: 3.2507\n",
      "Epoch 429/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.1630 - mean_squared_error: 3.1630\n",
      "Epoch 430/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.0063 - mean_squared_error: 3.0063\n",
      "Epoch 431/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.0024 - mean_squared_error: 3.0024\n",
      "Epoch 432/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.9427 - mean_squared_error: 2.9427\n",
      "Epoch 433/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.0183 - mean_squared_error: 3.0183\n",
      "Epoch 434/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.9037 - mean_squared_error: 2.9037\n",
      "Epoch 435/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.0042 - mean_squared_error: 3.0042\n",
      "Epoch 436/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.2695 - mean_squared_error: 3.2695\n",
      "Epoch 437/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.9140 - mean_squared_error: 2.9140\n",
      "Epoch 438/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 3.0602 - mean_squared_error: 3.0602\n",
      "Epoch 439/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 2.9756 - mean_squared_error: 2.9756\n",
      "Epoch 440/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.0777 - mean_squared_error: 3.0777\n",
      "Epoch 441/5000\n",
      "506/506 [==============================] - 0s 77us/step - loss: 3.1186 - mean_squared_error: 3.1186\n",
      "Epoch 442/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.1224 - mean_squared_error: 3.1224\n",
      "Epoch 443/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.0133 - mean_squared_error: 3.0133\n",
      "Epoch 444/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.9075 - mean_squared_error: 2.9075\n",
      "Epoch 445/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 2.9264 - mean_squared_error: 2.9264\n",
      "Epoch 446/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 2.8814 - mean_squared_error: 2.8814\n",
      "Epoch 447/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.1796 - mean_squared_error: 3.1796\n",
      "Epoch 448/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.9011 - mean_squared_error: 2.9011\n",
      "Epoch 449/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.9289 - mean_squared_error: 2.9289\n",
      "Epoch 450/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.8439 - mean_squared_error: 2.8439\n",
      "Epoch 451/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.9094 - mean_squared_error: 2.9094\n",
      "Epoch 452/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.9231 - mean_squared_error: 2.9231\n",
      "Epoch 453/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 2.7850 - mean_squared_error: 2.7850\n",
      "Epoch 454/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.9318 - mean_squared_error: 2.9318\n",
      "Epoch 455/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.9649 - mean_squared_error: 2.9649\n",
      "Epoch 456/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.8829 - mean_squared_error: 2.8829\n",
      "Epoch 457/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.7430 - mean_squared_error: 2.7430\n",
      "Epoch 458/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.8559 - mean_squared_error: 2.8559\n",
      "Epoch 459/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.7705 - mean_squared_error: 2.7705\n",
      "Epoch 460/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.7697 - mean_squared_error: 2.7697\n",
      "Epoch 461/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.7622 - mean_squared_error: 2.7622\n",
      "Epoch 462/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.7910 - mean_squared_error: 2.7910\n",
      "Epoch 463/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.8606 - mean_squared_error: 2.8606\n",
      "Epoch 464/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.9088 - mean_squared_error: 2.9088\n",
      "Epoch 465/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.7279 - mean_squared_error: 2.7279\n",
      "Epoch 466/5000\n",
      "506/506 [==============================] - 0s 92us/step - loss: 2.8337 - mean_squared_error: 2.8337\n",
      "Epoch 467/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.9335 - mean_squared_error: 2.9335\n",
      "Epoch 468/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.8651 - mean_squared_error: 2.8651\n",
      "Epoch 469/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 2.7673 - mean_squared_error: 2.7673\n",
      "Epoch 470/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.8904 - mean_squared_error: 2.8904\n",
      "Epoch 471/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.7775 - mean_squared_error: 2.7775\n",
      "Epoch 472/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.8505 - mean_squared_error: 2.8505\n",
      "Epoch 473/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.7999 - mean_squared_error: 2.7999\n",
      "Epoch 474/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.6780 - mean_squared_error: 2.6780\n",
      "Epoch 475/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.8578 - mean_squared_error: 2.8578\n",
      "Epoch 476/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 2.8339 - mean_squared_error: 2.8339\n",
      "Epoch 477/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.7691 - mean_squared_error: 2.7691\n",
      "Epoch 478/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 2.9702 - mean_squared_error: 2.9702\n",
      "Epoch 479/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.6533 - mean_squared_error: 2.6533\n",
      "Epoch 480/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.7802 - mean_squared_error: 2.7802\n",
      "Epoch 481/5000\n",
      "506/506 [==============================] - 0s 137us/step - loss: 2.7190 - mean_squared_error: 2.7190\n",
      "Epoch 482/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 2.7061 - mean_squared_error: 2.70610s - loss: 2.6766 - mean_squared_error: 2.67\n",
      "Epoch 483/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 2.6468 - mean_squared_error: 2.6468\n",
      "Epoch 484/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.6278 - mean_squared_error: 2.6278\n",
      "Epoch 485/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.7964 - mean_squared_error: 2.7964\n",
      "Epoch 486/5000\n",
      "506/506 [==============================] - 0s 173us/step - loss: 2.7622 - mean_squared_error: 2.7622\n",
      "Epoch 487/5000\n",
      "506/506 [==============================] - 0s 168us/step - loss: 2.7796 - mean_squared_error: 2.7796\n",
      "Epoch 488/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.7117 - mean_squared_error: 2.7117\n",
      "Epoch 489/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.6331 - mean_squared_error: 2.6331\n",
      "Epoch 490/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.6359 - mean_squared_error: 2.6359\n",
      "Epoch 491/5000\n",
      "506/506 [==============================] - 0s 73us/step - loss: 2.7250 - mean_squared_error: 2.7250\n",
      "Epoch 492/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 2.7507 - mean_squared_error: 2.7507\n",
      "Epoch 493/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 2.5784 - mean_squared_error: 2.5784\n",
      "Epoch 494/5000\n",
      "506/506 [==============================] - 0s 75us/step - loss: 2.6369 - mean_squared_error: 2.6369\n",
      "Epoch 495/5000\n",
      "506/506 [==============================] - 0s 81us/step - loss: 2.7713 - mean_squared_error: 2.7713\n",
      "Epoch 496/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.7691 - mean_squared_error: 2.7691\n",
      "Epoch 497/5000\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.6114 - mean_squared_error: 2.6114\n",
      "Epoch 498/5000\n",
      "506/506 [==============================] - 0s 79us/step - loss: 2.6407 - mean_squared_error: 2.6407\n",
      "Epoch 499/5000\n",
      "506/506 [==============================] - 0s 83us/step - loss: 2.6653 - mean_squared_error: 2.6653\n",
      "Epoch 500/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.5854 - mean_squared_error: 2.5854\n",
      "Epoch 501/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 2.5763 - mean_squared_error: 2.5763\n",
      "Epoch 502/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.7734 - mean_squared_error: 2.7734\n",
      "Epoch 503/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 2.6341 - mean_squared_error: 2.6341\n",
      "Epoch 504/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.8388 - mean_squared_error: 2.8388\n",
      "Epoch 505/5000\n",
      "506/506 [==============================] - 0s 91us/step - loss: 2.7392 - mean_squared_error: 2.7392\n",
      "Epoch 506/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.6501 - mean_squared_error: 2.6501\n",
      "Epoch 507/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.6018 - mean_squared_error: 2.6018\n",
      "Epoch 508/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.5982 - mean_squared_error: 2.5982\n",
      "Epoch 509/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.6597 - mean_squared_error: 2.6597\n",
      "Epoch 510/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.5757 - mean_squared_error: 2.5757\n",
      "Epoch 511/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4769 - mean_squared_error: 2.4769\n",
      "Epoch 512/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.5962 - mean_squared_error: 2.5962\n",
      "Epoch 513/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.5950 - mean_squared_error: 2.5950\n",
      "Epoch 514/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.6040 - mean_squared_error: 2.6040\n",
      "Epoch 515/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 2.5299 - mean_squared_error: 2.5299\n",
      "Epoch 516/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 2.5837 - mean_squared_error: 2.5837\n",
      "Epoch 517/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 2.6074 - mean_squared_error: 2.6074\n",
      "Epoch 518/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.5124 - mean_squared_error: 2.5124\n",
      "Epoch 519/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.6680 - mean_squared_error: 2.6680\n",
      "Epoch 520/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.6062 - mean_squared_error: 2.6062\n",
      "Epoch 521/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.4849 - mean_squared_error: 2.4849\n",
      "Epoch 522/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4378 - mean_squared_error: 2.4378\n",
      "Epoch 523/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.5178 - mean_squared_error: 2.5178\n",
      "Epoch 524/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 2.4789 - mean_squared_error: 2.4789\n",
      "Epoch 525/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 2.5652 - mean_squared_error: 2.5652\n",
      "Epoch 526/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 2.5015 - mean_squared_error: 2.5015\n",
      "Epoch 527/5000\n",
      "506/506 [==============================] - 0s 182us/step - loss: 2.5835 - mean_squared_error: 2.5835\n",
      "Epoch 528/5000\n",
      "506/506 [==============================] - 0s 195us/step - loss: 2.5264 - mean_squared_error: 2.5264\n",
      "Epoch 529/5000\n",
      "506/506 [==============================] - 0s 207us/step - loss: 2.5636 - mean_squared_error: 2.5636\n",
      "Epoch 530/5000\n",
      "506/506 [==============================] - 0s 210us/step - loss: 2.7548 - mean_squared_error: 2.7548\n",
      "Epoch 531/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 2.7144 - mean_squared_error: 2.7144\n",
      "Epoch 532/5000\n",
      "506/506 [==============================] - 0s 165us/step - loss: 2.5391 - mean_squared_error: 2.5391\n",
      "Epoch 533/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 2.6114 - mean_squared_error: 2.6114\n",
      "Epoch 534/5000\n",
      "506/506 [==============================] - 0s 182us/step - loss: 2.4450 - mean_squared_error: 2.4450\n",
      "Epoch 535/5000\n",
      "506/506 [==============================] - 0s 170us/step - loss: 2.4046 - mean_squared_error: 2.4046\n",
      "Epoch 536/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 2.4600 - mean_squared_error: 2.4600\n",
      "Epoch 537/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.5009 - mean_squared_error: 2.5009\n",
      "Epoch 538/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.4277 - mean_squared_error: 2.4277\n",
      "Epoch 539/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.6026 - mean_squared_error: 2.6026\n",
      "Epoch 540/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4103 - mean_squared_error: 2.4103\n",
      "Epoch 541/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4080 - mean_squared_error: 2.4080\n",
      "Epoch 542/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.5437 - mean_squared_error: 2.5437\n",
      "Epoch 543/5000\n",
      "506/506 [==============================] - 0s 148us/step - loss: 2.5295 - mean_squared_error: 2.5295\n",
      "Epoch 544/5000\n",
      "506/506 [==============================] - 0s 127us/step - loss: 2.5036 - mean_squared_error: 2.5036\n",
      "Epoch 545/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.5709 - mean_squared_error: 2.5709\n",
      "Epoch 546/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4623 - mean_squared_error: 2.4623\n",
      "Epoch 547/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4728 - mean_squared_error: 2.4728\n",
      "Epoch 548/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.3775 - mean_squared_error: 2.3775\n",
      "Epoch 549/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.4222 - mean_squared_error: 2.4222\n",
      "Epoch 550/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.4404 - mean_squared_error: 2.4404\n",
      "Epoch 551/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4038 - mean_squared_error: 2.4038\n",
      "Epoch 552/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.4102 - mean_squared_error: 2.4102\n",
      "Epoch 553/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.4886 - mean_squared_error: 2.4886\n",
      "Epoch 554/5000\n",
      "506/506 [==============================] - 0s 115us/step - loss: 2.3572 - mean_squared_error: 2.3572\n",
      "Epoch 555/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.3563 - mean_squared_error: 2.3563\n",
      "Epoch 556/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.5501 - mean_squared_error: 2.5501\n",
      "Epoch 557/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.4144 - mean_squared_error: 2.4144\n",
      "Epoch 558/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4353 - mean_squared_error: 2.4353\n",
      "Epoch 559/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.3502 - mean_squared_error: 2.3502\n",
      "Epoch 560/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.3661 - mean_squared_error: 2.3661\n",
      "Epoch 561/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4564 - mean_squared_error: 2.4564\n",
      "Epoch 562/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.4929 - mean_squared_error: 2.4929\n",
      "Epoch 563/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 128us/step - loss: 2.4462 - mean_squared_error: 2.4462\n",
      "Epoch 564/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.3606 - mean_squared_error: 2.3606\n",
      "Epoch 565/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.4082 - mean_squared_error: 2.4082\n",
      "Epoch 566/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 2.5076 - mean_squared_error: 2.5076\n",
      "Epoch 567/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.3700 - mean_squared_error: 2.3700\n",
      "Epoch 568/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.4080 - mean_squared_error: 2.4080\n",
      "Epoch 569/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.3511 - mean_squared_error: 2.3511\n",
      "Epoch 570/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.3716 - mean_squared_error: 2.3716\n",
      "Epoch 571/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.3742 - mean_squared_error: 2.3742\n",
      "Epoch 572/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.4577 - mean_squared_error: 2.4577\n",
      "Epoch 573/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.3743 - mean_squared_error: 2.3743\n",
      "Epoch 574/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.3727 - mean_squared_error: 2.3727\n",
      "Epoch 575/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 2.4306 - mean_squared_error: 2.4306\n",
      "Epoch 576/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.4382 - mean_squared_error: 2.4382\n",
      "Epoch 577/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.3429 - mean_squared_error: 2.3429\n",
      "Epoch 578/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.3687 - mean_squared_error: 2.3687\n",
      "Epoch 579/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 2.4418 - mean_squared_error: 2.4418\n",
      "Epoch 580/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 2.3212 - mean_squared_error: 2.3212\n",
      "Epoch 581/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.2759 - mean_squared_error: 2.2759\n",
      "Epoch 582/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.4483 - mean_squared_error: 2.4483\n",
      "Epoch 583/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.2928 - mean_squared_error: 2.2928\n",
      "Epoch 584/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.2952 - mean_squared_error: 2.2952\n",
      "Epoch 585/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.2662 - mean_squared_error: 2.2662\n",
      "Epoch 586/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.6151 - mean_squared_error: 2.6151\n",
      "Epoch 587/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.3029 - mean_squared_error: 2.3029\n",
      "Epoch 588/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.2359 - mean_squared_error: 2.2359\n",
      "Epoch 589/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.3779 - mean_squared_error: 2.3779\n",
      "Epoch 590/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.3837 - mean_squared_error: 2.3837\n",
      "Epoch 591/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.5015 - mean_squared_error: 2.5015\n",
      "Epoch 592/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.5147 - mean_squared_error: 2.5147\n",
      "Epoch 593/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.3760 - mean_squared_error: 2.3760\n",
      "Epoch 594/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.2674 - mean_squared_error: 2.2674\n",
      "Epoch 595/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.2821 - mean_squared_error: 2.2821\n",
      "Epoch 596/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 2.3351 - mean_squared_error: 2.3351\n",
      "Epoch 597/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.3020 - mean_squared_error: 2.3020\n",
      "Epoch 598/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.2909 - mean_squared_error: 2.2909\n",
      "Epoch 599/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.2553 - mean_squared_error: 2.2553\n",
      "Epoch 600/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.2383 - mean_squared_error: 2.2383\n",
      "Epoch 601/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 2.2125 - mean_squared_error: 2.2125\n",
      "Epoch 602/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.3424 - mean_squared_error: 2.3424\n",
      "Epoch 603/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4540 - mean_squared_error: 2.4540\n",
      "Epoch 604/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.4321 - mean_squared_error: 2.4321\n",
      "Epoch 605/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.2399 - mean_squared_error: 2.2399\n",
      "Epoch 606/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.3658 - mean_squared_error: 2.3658\n",
      "Epoch 607/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1319 - mean_squared_error: 2.1319\n",
      "Epoch 608/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.2110 - mean_squared_error: 2.2110\n",
      "Epoch 609/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.2330 - mean_squared_error: 2.23 - 0s 126us/step - loss: 2.1941 - mean_squared_error: 2.1941\n",
      "Epoch 610/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.2484 - mean_squared_error: 2.2484\n",
      "Epoch 611/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 2.2656 - mean_squared_error: 2.2656\n",
      "Epoch 612/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.1648 - mean_squared_error: 2.1648\n",
      "Epoch 613/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4042 - mean_squared_error: 2.4042\n",
      "Epoch 614/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.3835 - mean_squared_error: 2.3835\n",
      "Epoch 615/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.1507 - mean_squared_error: 2.1507\n",
      "Epoch 616/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1430 - mean_squared_error: 2.1430\n",
      "Epoch 617/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.2154 - mean_squared_error: 2.2154\n",
      "Epoch 618/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.2118 - mean_squared_error: 2.2118\n",
      "Epoch 619/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.1650 - mean_squared_error: 2.1650\n",
      "Epoch 620/5000\n",
      "506/506 [==============================] - 0s 133us/step - loss: 2.2872 - mean_squared_error: 2.2872\n",
      "Epoch 621/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 2.1796 - mean_squared_error: 2.1796\n",
      "Epoch 622/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 2.1606 - mean_squared_error: 2.1606\n",
      "Epoch 623/5000\n",
      "506/506 [==============================] - 0s 125us/step - loss: 2.2207 - mean_squared_error: 2.2207\n",
      "Epoch 624/5000\n",
      "506/506 [==============================] - 0s 121us/step - loss: 2.3818 - mean_squared_error: 2.3818\n",
      "Epoch 625/5000\n",
      "506/506 [==============================] - 0s 117us/step - loss: 2.1968 - mean_squared_error: 2.1968\n",
      "Epoch 626/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.1448 - mean_squared_error: 2.1448\n",
      "Epoch 627/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.1691 - mean_squared_error: 2.1691\n",
      "Epoch 628/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.1621 - mean_squared_error: 2.1621\n",
      "Epoch 629/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.1523 - mean_squared_error: 2.1523\n",
      "Epoch 630/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.3014 - mean_squared_error: 2.3014\n",
      "Epoch 631/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.3916 - mean_squared_error: 1.39 - 0s 112us/step - loss: 2.1765 - mean_squared_error: 2.1765\n",
      "Epoch 632/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.1081 - mean_squared_error: 2.1081\n",
      "Epoch 633/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 104us/step - loss: 2.0759 - mean_squared_error: 2.0759\n",
      "Epoch 634/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 2.1992 - mean_squared_error: 2.1992\n",
      "Epoch 635/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.3063 - mean_squared_error: 2.3063\n",
      "Epoch 636/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.2712 - mean_squared_error: 2.2712\n",
      "Epoch 637/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.1127 - mean_squared_error: 2.1127\n",
      "Epoch 638/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.0987 - mean_squared_error: 2.0987\n",
      "Epoch 639/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.1473 - mean_squared_error: 2.1473\n",
      "Epoch 640/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.1416 - mean_squared_error: 2.1416\n",
      "Epoch 641/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1361 - mean_squared_error: 2.1361\n",
      "Epoch 642/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.1291 - mean_squared_error: 2.12910s - loss: 2.0487 - mean_squared_error: 2.04\n",
      "Epoch 643/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.1180 - mean_squared_error: 2.1180\n",
      "Epoch 644/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.2402 - mean_squared_error: 2.2402\n",
      "Epoch 645/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.2545 - mean_squared_error: 2.2545\n",
      "Epoch 646/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1604 - mean_squared_error: 2.1604\n",
      "Epoch 647/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0894 - mean_squared_error: 2.0894\n",
      "Epoch 648/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.1877 - mean_squared_error: 2.1877\n",
      "Epoch 649/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.3832 - mean_squared_error: 2.3832\n",
      "Epoch 650/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 2.1370 - mean_squared_error: 2.1370\n",
      "Epoch 651/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.1087 - mean_squared_error: 2.1087\n",
      "Epoch 652/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.1594 - mean_squared_error: 2.1594\n",
      "Epoch 653/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0412 - mean_squared_error: 2.0412\n",
      "Epoch 654/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0834 - mean_squared_error: 2.0834\n",
      "Epoch 655/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.0991 - mean_squared_error: 2.0991\n",
      "Epoch 656/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.0112 - mean_squared_error: 2.0112\n",
      "Epoch 657/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.1252 - mean_squared_error: 2.1252\n",
      "Epoch 658/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1105 - mean_squared_error: 2.1105\n",
      "Epoch 659/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.0179 - mean_squared_error: 2.0179\n",
      "Epoch 660/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0858 - mean_squared_error: 2.0858\n",
      "Epoch 661/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.1181 - mean_squared_error: 2.1181\n",
      "Epoch 662/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.1793 - mean_squared_error: 2.1793\n",
      "Epoch 663/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.1338 - mean_squared_error: 2.1338\n",
      "Epoch 664/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.2241 - mean_squared_error: 2.2241\n",
      "Epoch 665/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.0850 - mean_squared_error: 2.0850\n",
      "Epoch 666/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.1735 - mean_squared_error: 2.1735\n",
      "Epoch 667/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.0284 - mean_squared_error: 2.0284\n",
      "Epoch 668/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.0583 - mean_squared_error: 2.0583\n",
      "Epoch 669/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.0636 - mean_squared_error: 2.0636\n",
      "Epoch 670/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0652 - mean_squared_error: 2.0652\n",
      "Epoch 671/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0345 - mean_squared_error: 2.0345\n",
      "Epoch 672/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0263 - mean_squared_error: 2.0263\n",
      "Epoch 673/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1038 - mean_squared_error: 2.1038\n",
      "Epoch 674/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 2.0847 - mean_squared_error: 2.0847\n",
      "Epoch 675/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.2208 - mean_squared_error: 2.2208\n",
      "Epoch 676/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.1628 - mean_squared_error: 2.1628\n",
      "Epoch 677/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.9884 - mean_squared_error: 1.9884\n",
      "Epoch 678/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.1562 - mean_squared_error: 2.1562\n",
      "Epoch 679/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0510 - mean_squared_error: 2.0510\n",
      "Epoch 680/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0788 - mean_squared_error: 2.0788\n",
      "Epoch 681/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.9946 - mean_squared_error: 1.9946\n",
      "Epoch 682/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0788 - mean_squared_error: 2.0788\n",
      "Epoch 683/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0300 - mean_squared_error: 2.0300\n",
      "Epoch 684/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.0111 - mean_squared_error: 2.0111\n",
      "Epoch 685/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0944 - mean_squared_error: 2.0944\n",
      "Epoch 686/5000\n",
      "506/506 [==============================] - 0s 94us/step - loss: 2.0509 - mean_squared_error: 2.0509\n",
      "Epoch 687/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.1474 - mean_squared_error: 2.1474\n",
      "Epoch 688/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0277 - mean_squared_error: 2.0277\n",
      "Epoch 689/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.5507 - mean_squared_error: 2.5507\n",
      "Epoch 690/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.0141 - mean_squared_error: 2.0141\n",
      "Epoch 691/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1716 - mean_squared_error: 2.1716\n",
      "Epoch 692/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.9724 - mean_squared_error: 1.9724\n",
      "Epoch 693/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.9981 - mean_squared_error: 1.9981\n",
      "Epoch 694/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.0999 - mean_squared_error: 2.0999\n",
      "Epoch 695/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.1396 - mean_squared_error: 2.1396\n",
      "Epoch 696/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.1207 - mean_squared_error: 2.1207\n",
      "Epoch 697/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.1248 - mean_squared_error: 2.1248\n",
      "Epoch 698/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.9632 - mean_squared_error: 1.9632\n",
      "Epoch 699/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 1.9622 - mean_squared_error: 1.9622\n",
      "Epoch 700/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.0137 - mean_squared_error: 2.0137\n",
      "Epoch 701/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.9930 - mean_squared_error: 1.9930\n",
      "Epoch 702/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 2.2429 - mean_squared_error: 2.2429\n",
      "Epoch 703/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.0635 - mean_squared_error: 2.0635\n",
      "Epoch 704/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.0087 - mean_squared_error: 2.0087\n",
      "Epoch 705/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.0311 - mean_squared_error: 2.0311\n",
      "Epoch 706/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.1968 - mean_squared_error: 2.1968\n",
      "Epoch 707/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.0284 - mean_squared_error: 2.0284\n",
      "Epoch 708/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1761 - mean_squared_error: 2.1761\n",
      "Epoch 709/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.9897 - mean_squared_error: 1.9897\n",
      "Epoch 710/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.9476 - mean_squared_error: 1.9476\n",
      "Epoch 711/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.0501 - mean_squared_error: 2.0501\n",
      "Epoch 712/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 2.0490 - mean_squared_error: 2.0490\n",
      "Epoch 713/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.9787 - mean_squared_error: 1.9787\n",
      "Epoch 714/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9861 - mean_squared_error: 1.9861\n",
      "Epoch 715/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.9911 - mean_squared_error: 1.9911\n",
      "Epoch 716/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.9810 - mean_squared_error: 1.9810\n",
      "Epoch 717/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.9143 - mean_squared_error: 1.9143\n",
      "Epoch 718/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.9883 - mean_squared_error: 1.9883\n",
      "Epoch 719/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.0338 - mean_squared_error: 2.0338\n",
      "Epoch 720/5000\n",
      "506/506 [==============================] - 0s 133us/step - loss: 1.9354 - mean_squared_error: 1.9354\n",
      "Epoch 721/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.9609 - mean_squared_error: 1.96090s - loss: 1.9700 - mean_squared_error: 1.97\n",
      "Epoch 722/5000\n",
      "506/506 [==============================] - 0s 98us/step - loss: 2.0647 - mean_squared_error: 2.0647\n",
      "Epoch 723/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1468 - mean_squared_error: 2.1468\n",
      "Epoch 724/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0610 - mean_squared_error: 2.0610\n",
      "Epoch 725/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.9396 - mean_squared_error: 1.9396\n",
      "Epoch 726/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.0093 - mean_squared_error: 2.0093\n",
      "Epoch 727/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1029 - mean_squared_error: 2.1029\n",
      "Epoch 728/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.1019 - mean_squared_error: 2.1019\n",
      "Epoch 729/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0140 - mean_squared_error: 2.0140\n",
      "Epoch 730/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.9553 - mean_squared_error: 1.9553\n",
      "Epoch 731/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.9611 - mean_squared_error: 1.9611\n",
      "Epoch 732/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 2.0181 - mean_squared_error: 2.0181\n",
      "Epoch 733/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.9226 - mean_squared_error: 1.9226\n",
      "Epoch 734/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0060 - mean_squared_error: 2.0060\n",
      "Epoch 735/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 2.1644 - mean_squared_error: 2.1644\n",
      "Epoch 736/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.9255 - mean_squared_error: 1.9255\n",
      "Epoch 737/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9713 - mean_squared_error: 1.9713\n",
      "Epoch 738/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.9056 - mean_squared_error: 1.9056\n",
      "Epoch 739/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9388 - mean_squared_error: 1.9388\n",
      "Epoch 740/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.8950 - mean_squared_error: 1.8950\n",
      "Epoch 741/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.9403 - mean_squared_error: 1.9403\n",
      "Epoch 742/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.0414 - mean_squared_error: 2.0414\n",
      "Epoch 743/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.9025 - mean_squared_error: 1.9025\n",
      "Epoch 744/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8533 - mean_squared_error: 1.8533\n",
      "Epoch 745/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8816 - mean_squared_error: 1.8816\n",
      "Epoch 746/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.9269 - mean_squared_error: 1.9269\n",
      "Epoch 747/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.9974 - mean_squared_error: 1.9974\n",
      "Epoch 748/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.9321 - mean_squared_error: 1.9321\n",
      "Epoch 749/5000\n",
      "506/506 [==============================] - 0s 123us/step - loss: 1.9878 - mean_squared_error: 1.9878\n",
      "Epoch 750/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.9325 - mean_squared_error: 1.9325\n",
      "Epoch 751/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.9230 - mean_squared_error: 1.9230\n",
      "Epoch 752/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.8929 - mean_squared_error: 1.8929\n",
      "Epoch 753/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9134 - mean_squared_error: 1.9134\n",
      "Epoch 754/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9031 - mean_squared_error: 1.9031\n",
      "Epoch 755/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.8154 - mean_squared_error: 1.8154\n",
      "Epoch 756/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.8834 - mean_squared_error: 1.8834\n",
      "Epoch 757/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.8320 - mean_squared_error: 1.8320\n",
      "Epoch 758/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.8449 - mean_squared_error: 1.8449\n",
      "Epoch 759/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.9361 - mean_squared_error: 1.9361\n",
      "Epoch 760/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.8597 - mean_squared_error: 1.8597\n",
      "Epoch 761/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.9245 - mean_squared_error: 1.9245\n",
      "Epoch 762/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.9958 - mean_squared_error: 1.9958\n",
      "Epoch 763/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8836 - mean_squared_error: 1.8836\n",
      "Epoch 764/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9758 - mean_squared_error: 1.9758\n",
      "Epoch 765/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.9611 - mean_squared_error: 1.9611\n",
      "Epoch 766/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 1.8815 - mean_squared_error: 1.8815\n",
      "Epoch 767/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8504 - mean_squared_error: 1.8504\n",
      "Epoch 768/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7979 - mean_squared_error: 1.7979\n",
      "Epoch 769/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.8596 - mean_squared_error: 1.8596\n",
      "Epoch 770/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.9311 - mean_squared_error: 1.9311\n",
      "Epoch 771/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.8126 - mean_squared_error: 1.8126\n",
      "Epoch 772/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.8158 - mean_squared_error: 1.8158\n",
      "Epoch 773/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 97us/step - loss: 1.8546 - mean_squared_error: 1.8546\n",
      "Epoch 774/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8529 - mean_squared_error: 1.8529\n",
      "Epoch 775/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.8946 - mean_squared_error: 1.8946\n",
      "Epoch 776/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8096 - mean_squared_error: 1.8096\n",
      "Epoch 777/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8529 - mean_squared_error: 1.8529\n",
      "Epoch 778/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8318 - mean_squared_error: 1.8318\n",
      "Epoch 779/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8743 - mean_squared_error: 1.8743\n",
      "Epoch 780/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8446 - mean_squared_error: 1.8446\n",
      "Epoch 781/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.8707 - mean_squared_error: 1.8707\n",
      "Epoch 782/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8181 - mean_squared_error: 1.8181\n",
      "Epoch 783/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 1.8436 - mean_squared_error: 1.8436\n",
      "Epoch 784/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8110 - mean_squared_error: 1.8110\n",
      "Epoch 785/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8733 - mean_squared_error: 1.8733\n",
      "Epoch 786/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.8582 - mean_squared_error: 1.8582\n",
      "Epoch 787/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0168 - mean_squared_error: 2.0168\n",
      "Epoch 788/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8975 - mean_squared_error: 1.8975\n",
      "Epoch 789/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8579 - mean_squared_error: 1.8579\n",
      "Epoch 790/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.0719 - mean_squared_error: 2.0719\n",
      "Epoch 791/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8599 - mean_squared_error: 1.8599\n",
      "Epoch 792/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.8441 - mean_squared_error: 1.8441\n",
      "Epoch 793/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.0148 - mean_squared_error: 2.0148\n",
      "Epoch 794/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8473 - mean_squared_error: 1.8473\n",
      "Epoch 795/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8266 - mean_squared_error: 1.8266\n",
      "Epoch 796/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.7728 - mean_squared_error: 1.7728\n",
      "Epoch 797/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8706 - mean_squared_error: 1.8706\n",
      "Epoch 798/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 1.8117 - mean_squared_error: 1.8117\n",
      "Epoch 799/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.9863 - mean_squared_error: 1.9863\n",
      "Epoch 800/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.7770 - mean_squared_error: 1.7770\n",
      "Epoch 801/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.7835 - mean_squared_error: 1.7835\n",
      "Epoch 802/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7713 - mean_squared_error: 1.7713\n",
      "Epoch 803/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8562 - mean_squared_error: 1.8562\n",
      "Epoch 804/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8281 - mean_squared_error: 1.8281\n",
      "Epoch 805/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8301 - mean_squared_error: 1.8301\n",
      "Epoch 806/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.7488 - mean_squared_error: 1.7488\n",
      "Epoch 807/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.7832 - mean_squared_error: 1.7832\n",
      "Epoch 808/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.8701 - mean_squared_error: 1.8701\n",
      "Epoch 809/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7845 - mean_squared_error: 1.7845\n",
      "Epoch 810/5000\n",
      "506/506 [==============================] - 0s 100us/step - loss: 1.7205 - mean_squared_error: 1.7205\n",
      "Epoch 811/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7792 - mean_squared_error: 1.7792\n",
      "Epoch 812/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.7892 - mean_squared_error: 1.7892\n",
      "Epoch 813/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7937 - mean_squared_error: 1.7937\n",
      "Epoch 814/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8549 - mean_squared_error: 1.8549\n",
      "Epoch 815/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.8205 - mean_squared_error: 1.8205\n",
      "Epoch 816/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.7372 - mean_squared_error: 1.7372\n",
      "Epoch 817/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.8619 - mean_squared_error: 1.8619\n",
      "Epoch 818/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.8225 - mean_squared_error: 1.8225\n",
      "Epoch 819/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.7450 - mean_squared_error: 1.7450\n",
      "Epoch 820/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7323 - mean_squared_error: 1.7323\n",
      "Epoch 821/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.7494 - mean_squared_error: 1.7494\n",
      "Epoch 822/5000\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.8664 - mean_squared_error: 1.8664\n",
      "Epoch 823/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.9156 - mean_squared_error: 1.9156\n",
      "Epoch 824/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.7658 - mean_squared_error: 1.7658\n",
      "Epoch 825/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.8946 - mean_squared_error: 1.8946\n",
      "Epoch 826/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.7696 - mean_squared_error: 1.7696\n",
      "Epoch 827/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.8350 - mean_squared_error: 1.83500s - loss: 1.8656 - mean_squared_error: 1.86\n",
      "Epoch 828/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.7667 - mean_squared_error: 1.7667\n",
      "Epoch 829/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7993 - mean_squared_error: 1.7993\n",
      "Epoch 830/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8434 - mean_squared_error: 1.8434\n",
      "Epoch 831/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.8779 - mean_squared_error: 1.8779\n",
      "Epoch 832/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.8503 - mean_squared_error: 1.8503\n",
      "Epoch 833/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.7993 - mean_squared_error: 1.7993\n",
      "Epoch 834/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.9128 - mean_squared_error: 1.9128\n",
      "Epoch 835/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.7430 - mean_squared_error: 1.7430\n",
      "Epoch 836/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.7440 - mean_squared_error: 1.7440\n",
      "Epoch 837/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.7636 - mean_squared_error: 1.7636\n",
      "Epoch 838/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7474 - mean_squared_error: 1.7474\n",
      "Epoch 839/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7986 - mean_squared_error: 1.7986\n",
      "Epoch 840/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.7343 - mean_squared_error: 1.7343\n",
      "Epoch 841/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7392 - mean_squared_error: 1.7392\n",
      "Epoch 842/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7742 - mean_squared_error: 1.7742\n",
      "Epoch 843/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.8436 - mean_squared_error: 1.8436\n",
      "Epoch 844/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.7418 - mean_squared_error: 1.7418\n",
      "Epoch 845/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.7448 - mean_squared_error: 1.7448\n",
      "Epoch 846/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.7178 - mean_squared_error: 1.7178\n",
      "Epoch 847/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.7389 - mean_squared_error: 1.7389\n",
      "Epoch 848/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.8473 - mean_squared_error: 1.8473\n",
      "Epoch 849/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.8297 - mean_squared_error: 1.8297\n",
      "Epoch 850/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.7184 - mean_squared_error: 1.7184\n",
      "Epoch 851/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.8051 - mean_squared_error: 1.8051\n",
      "Epoch 852/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7243 - mean_squared_error: 1.7243\n",
      "Epoch 853/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6830 - mean_squared_error: 1.6830\n",
      "Epoch 854/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.7322 - mean_squared_error: 1.7322\n",
      "Epoch 855/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.9241 - mean_squared_error: 1.9241\n",
      "Epoch 856/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.7617 - mean_squared_error: 1.7617\n",
      "Epoch 857/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 2.6031 - mean_squared_error: 2.60 - 0s 108us/step - loss: 1.7393 - mean_squared_error: 1.7393\n",
      "Epoch 858/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7118 - mean_squared_error: 1.7118\n",
      "Epoch 859/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.6958 - mean_squared_error: 1.6958\n",
      "Epoch 860/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7361 - mean_squared_error: 1.7361\n",
      "Epoch 861/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.6822 - mean_squared_error: 1.6822\n",
      "Epoch 862/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7009 - mean_squared_error: 1.7009\n",
      "Epoch 863/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.7450 - mean_squared_error: 1.7450\n",
      "Epoch 864/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7686 - mean_squared_error: 1.7686\n",
      "Epoch 865/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.7436 - mean_squared_error: 1.7436\n",
      "Epoch 866/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.8369 - mean_squared_error: 1.8369\n",
      "Epoch 867/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.7259 - mean_squared_error: 1.7259\n",
      "Epoch 868/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.6933 - mean_squared_error: 1.6933\n",
      "Epoch 869/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.7823 - mean_squared_error: 1.7823\n",
      "Epoch 870/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.7320 - mean_squared_error: 1.7320\n",
      "Epoch 871/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.7217 - mean_squared_error: 1.7217\n",
      "Epoch 872/5000\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.7812 - mean_squared_error: 1.7812\n",
      "Epoch 873/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.8950 - mean_squared_error: 1.8950\n",
      "Epoch 874/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.7298 - mean_squared_error: 1.7298\n",
      "Epoch 875/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.6761 - mean_squared_error: 1.6761\n",
      "Epoch 876/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.6333 - mean_squared_error: 1.6333\n",
      "Epoch 877/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.6815 - mean_squared_error: 1.6815\n",
      "Epoch 878/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.6954 - mean_squared_error: 1.6954\n",
      "Epoch 879/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.6642 - mean_squared_error: 1.6642\n",
      "Epoch 880/5000\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.7089 - mean_squared_error: 1.7089\n",
      "Epoch 881/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.6506 - mean_squared_error: 1.6506\n",
      "Epoch 882/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6715 - mean_squared_error: 1.6715\n",
      "Epoch 883/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.6715 - mean_squared_error: 1.6715\n",
      "Epoch 884/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.6921 - mean_squared_error: 1.6921\n",
      "Epoch 885/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.7449 - mean_squared_error: 1.7449\n",
      "Epoch 886/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 1.8214 - mean_squared_error: 1.8214\n",
      "Epoch 887/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.6886 - mean_squared_error: 1.6886\n",
      "Epoch 888/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.6604 - mean_squared_error: 1.6604\n",
      "Epoch 889/5000\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.6928 - mean_squared_error: 1.6928\n",
      "Epoch 890/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.6782 - mean_squared_error: 1.6782\n",
      "Epoch 891/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7202 - mean_squared_error: 1.7202\n",
      "Epoch 892/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.6813 - mean_squared_error: 1.6813\n",
      "Epoch 893/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7045 - mean_squared_error: 1.7045\n",
      "Epoch 894/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6683 - mean_squared_error: 1.6683\n",
      "Epoch 895/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6731 - mean_squared_error: 1.6731\n",
      "Epoch 896/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6583 - mean_squared_error: 1.6583\n",
      "Epoch 897/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6589 - mean_squared_error: 1.6589\n",
      "Epoch 898/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.6828 - mean_squared_error: 1.6828\n",
      "Epoch 899/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.6789 - mean_squared_error: 1.6789\n",
      "Epoch 900/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.6801 - mean_squared_error: 1.6801\n",
      "Epoch 901/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.7280 - mean_squared_error: 1.7280\n",
      "Epoch 902/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7153 - mean_squared_error: 1.7153\n",
      "Epoch 903/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 1.6716 - mean_squared_error: 1.6716\n",
      "Epoch 904/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.6617 - mean_squared_error: 1.6617\n",
      "Epoch 905/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7032 - mean_squared_error: 1.7032\n",
      "Epoch 906/5000\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.6321 - mean_squared_error: 1.6321\n",
      "Epoch 907/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6447 - mean_squared_error: 1.6447\n",
      "Epoch 908/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.6755 - mean_squared_error: 1.6755\n",
      "Epoch 909/5000\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.7693 - mean_squared_error: 1.7693\n",
      "Epoch 910/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.6539 - mean_squared_error: 1.6539\n",
      "Epoch 911/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6332 - mean_squared_error: 1.6332\n",
      "Epoch 912/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.6141 - mean_squared_error: 1.6141\n",
      "Epoch 913/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 102us/step - loss: 1.6820 - mean_squared_error: 1.6820\n",
      "Epoch 914/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.6497 - mean_squared_error: 1.6497\n",
      "Epoch 915/5000\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.6529 - mean_squared_error: 1.6529\n",
      "Epoch 916/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6749 - mean_squared_error: 1.6749\n",
      "Epoch 917/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 1.3106 - mean_squared_error: 1.31 - 0s 106us/step - loss: 1.6388 - mean_squared_error: 1.6388\n",
      "Epoch 918/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7046 - mean_squared_error: 1.7046\n",
      "Epoch 919/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6776 - mean_squared_error: 1.6776\n",
      "Epoch 920/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6983 - mean_squared_error: 1.6983\n",
      "Epoch 921/5000\n",
      "506/506 [==============================] - 0s 89us/step - loss: 1.5920 - mean_squared_error: 1.5920\n",
      "Epoch 922/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6983 - mean_squared_error: 1.6983\n",
      "Epoch 923/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6325 - mean_squared_error: 1.6325\n",
      "Epoch 924/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6070 - mean_squared_error: 1.6070\n",
      "Epoch 925/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.6923 - mean_squared_error: 1.6923\n",
      "Epoch 926/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.7328 - mean_squared_error: 1.7328\n",
      "Epoch 927/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.6111 - mean_squared_error: 1.6111\n",
      "Epoch 928/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6772 - mean_squared_error: 1.6772\n",
      "Epoch 929/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.6081 - mean_squared_error: 1.6081\n",
      "Epoch 930/5000\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.6114 - mean_squared_error: 1.6114\n",
      "Epoch 931/5000\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7613 - mean_squared_error: 1.7613\n",
      "Epoch 932/5000\n",
      "506/506 [==============================] - 0s 168us/step - loss: 1.6066 - mean_squared_error: 1.6066\n",
      "Epoch 933/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.6001 - mean_squared_error: 1.6001\n",
      "Epoch 934/5000\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.5946 - mean_squared_error: 1.5946\n",
      "Epoch 935/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.6381 - mean_squared_error: 1.6381\n",
      "Epoch 936/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.6079 - mean_squared_error: 1.6079\n",
      "Epoch 937/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.6646 - mean_squared_error: 1.6646\n",
      "Epoch 938/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.6455 - mean_squared_error: 1.6455\n",
      "Epoch 939/5000\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.5704 - mean_squared_error: 1.5704\n",
      "Epoch 940/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7638 - mean_squared_error: 1.7638\n",
      "Epoch 941/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.5639 - mean_squared_error: 1.5639\n",
      "Epoch 942/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6348 - mean_squared_error: 1.6348\n",
      "Epoch 943/5000\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6762 - mean_squared_error: 1.6762\n",
      "Epoch 944/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7321 - mean_squared_error: 1.7321\n",
      "Epoch 945/5000\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.5846 - mean_squared_error: 1.5846\n",
      "Epoch 946/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6131 - mean_squared_error: 1.6131\n",
      "Epoch 947/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.5554 - mean_squared_error: 1.5554\n",
      "Epoch 948/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6773 - mean_squared_error: 1.6773\n",
      "Epoch 949/5000\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.7313 - mean_squared_error: 1.7313\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=5000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A neuron computes a linear function followed by an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Because linear functions can not express enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Same as cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There would be no loss convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x,axis):# 列向量axis=0，否则1\n",
    "    exp_x = np.exp(x)\n",
    "    sum_x = np.sum(exp_x,axis,keepdims = True)\n",
    "    return exp_x/sum_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷是 Windows\n",
      " 卷的序列号是 1CB4-D8AE\n",
      "\n",
      " C:\\Users\\86189\\Desktop\\机器学习\\pynb\\kaikeba 的目录\n",
      "\n",
      "2020/02/19  21:25    <DIR>          .\n",
      "2020/02/19  21:25    <DIR>          ..\n",
      "2020/02/09  23:29    <DIR>          .ipynb_checkpoints\n",
      "2020/01/27  15:06            29,176 assignment-01-optional-pattern-match.ipynb\n",
      "2020/02/13  14:34            93,807 Assignment-03(1).ipynb\n",
      "2020/02/19  21:25           752,596 Assignment_4.ipynb\n",
      "2020/01/29  16:13            54,750 chatbot-duan03.ipynb\n",
      "2020/01/23  15:03            75,241 Lecture-01-Syntax-Tree-and-Language-Model.ipynb课程在线准备代码.ipynb\n",
      "2020/01/27  15:12            61,075 lecture1(duan).ipynb\n",
      "2020/01/28  21:06            51,838 lecture1（duan）-2.ipynb\n",
      "2020/01/29  20:52           245,138 Lecture_02.ipynb\n",
      "2020/01/26  18:21    <DIR>          lesson-03-course（1）\n",
      "2020/02/04  17:00            98,113 lesson-03-course（1）.ipynb\n",
      "2020/02/09  17:48            84,492 lesson-03-pre-course(1).ipynb\n",
      "2020/02/09  23:29            25,864 Lesson-05.ipynb\n",
      "2020/02/16  18:07           157,739 Lesson-4代码.ipynb\n",
      "2020/01/26  18:11           106,712 NLP-01-周燕.ipynb\n",
      "2020/02/09  16:45           635,156 NLP-02-周燕.ipynb\n",
      "2020/02/09  17:11           429,517 NLP-02-周燕.zip\n",
      "2020/02/13  14:31            93,807 NLP-03-周燕.ipynb\n",
      "2020/02/13  14:42            41,043 NLP-03-周燕.zip\n",
      "              17 个文件      3,036,064 字节\n",
      "               4 个目录 11,818,500,096 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1/(1+np.exp(-1*z)) \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.rand(dim,1)*0.01 #np.zeros(dim)\n",
    "    b = 0 #np.zeros(dim)\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.rand(5,1)*0.01).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    X = X.T\n",
    "    m = X.shape[0]\n",
    "    A = sigmoid(np.dot(w.T,X)+ b) \n",
    "    cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    dw = 1/m*np.dot(X,(A-Y).T)\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w-dw*learning_rate\n",
    "        b = b -db*learning_rate\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''      \n",
    "    m = X.shape[0]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    # w = w.reshape(1,-1)\n",
    "    #print('w,X:',w.shape,X.shape)\n",
    "    A = sigmoid(np.dot(w.T,X.T) +b)\n",
    "\n",
    "    # print('shape of A:',A.shape)\n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0,i] = A[0,i]>0.5 \n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,real):\n",
    "    pred=pred[0,:]\n",
    "    #real = real[0,:]\n",
    "    return sum((pred-real)==0)/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.33 %\n"
     ]
    }
   ],
   "source": [
    "print('%.2f'%(sum(np.array([1,0,1])-np.array([0,1,1])==0)/len(np.array([0,1,1]))*100),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    w,b = initialize_parameters(X_train.shape[1])\n",
    "    p,g,cost=optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    # print(p,g,cost)\n",
    "    #print('shape of p,b:',p['w'].shape,p['b'].shape)\n",
    "    train_y_pred = predict(p['w'],p['b'],X_train) \n",
    "    #print('shape of p,b:',p['w'].shape,p['b'].shape)\n",
    "    Y_prediction = predict(p['w'],p['b'],X_test)\n",
    "    \n",
    "    d = {\"w\":p['w'],\n",
    "         \"b\":p['b'],\n",
    "         \"training_accuracy\": accuracy(train_y_pred,Y_train),\n",
    "         \"test_accuracy\": accuracy(Y_prediction,Y_test),\n",
    "         \"cost\":cost}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9146250927988122\n",
      "testing accuracy: 0.8911111111111111\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train,y_train,X_test,y_test,8000,0.001,True)\n",
    "print('training accuracy:',d['training_accuracy'])\n",
    "print('testing accuracy:',d['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
